{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Điều 1, 2, 3 + replace\n",
    "import re\n",
    "\n",
    "def split_by_dieu_new1(text):\n",
    "    # def extract_dieu_positions(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "\n",
    "\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in dieu_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "################\n",
    "def extract_dieu_err(text):\n",
    "    # Define the pattern to match \"Điều\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "##############\n",
    "def process_dieu_parts(text):\n",
    "    dieu_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)Điều\\s+\\d+', temp):\n",
    "                    dieu_final.append(temp)\n",
    "                else:\n",
    "                    dieu_final.append('Điều ' + temp)\n",
    "\n",
    "    return dieu_final\n",
    "###############\n",
    "def replace_dieu_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_dieu_new1(text)\n",
    "    matches = extract_dieu_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    dieu_final = process_dieu_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)Điều\\s+\\d+', part) and \" và \" in part:\n",
    "            for dieu in dieu_final:\n",
    "                replaced_text.append(dieu)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "\n",
    "#############\n",
    "#############\n",
    "########### khoản 1, 2,3  và .... + replace\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def split_by_khoan_new1(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in khoan_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "\n",
    "#####################\n",
    "def extract_khoan_err(text):\n",
    "    # Define the pattern to match \"khoản\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "################\n",
    "def process_khoan_parts(text):\n",
    "    khoan_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    khoan_final.append(temp)\n",
    "                else:\n",
    "                    khoan_final.append('khoản ' + temp)\n",
    "\n",
    "    return khoan_final\n",
    "###############\n",
    "\n",
    "def replace_khoan_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_khoan_new1(text)\n",
    "    matches = extract_khoan_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    khoan_final = process_khoan_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for khoan in khoan_final:\n",
    "                replaced_text.append(khoan)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "#############\n",
    "#############\n",
    "################# Điểm 1,2,3 và .... + replace\n",
    "import re\n",
    "\n",
    "def split_by_diem_new1(text):\n",
    "    def extract_diem_positions(text):\n",
    "        return [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])', text, re.IGNORECASE)]\n",
    "\n",
    "    diem_positions = extract_diem_positions(text)\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in diem_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "###########################\n",
    "def extract_diem_err(text):\n",
    "    # Define the pattern to match \"điểm\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "#########################\n",
    "def process_diem_parts(text):\n",
    "    diem_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    diem_final.append(temp)\n",
    "                else:\n",
    "                    diem_final.append('điểm ' + temp)\n",
    "\n",
    "    return diem_final\n",
    "#########################\n",
    "def replace_diem_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_diem_new1(text)\n",
    "    matches = extract_diem_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    diem_final = process_diem_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for diem in diem_final:\n",
    "                replaced_text.append(diem)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_transform_dieu_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_dieu = None\n",
    "\n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ Điều\" in split_text_part:\n",
    "                start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if start_dieu_match:\n",
    "                    start_dieu = start_dieu_match.group(1)\n",
    "            elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "                end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if end_dieu_match:\n",
    "                    end_dieu = end_dieu_match.group(1)\n",
    "                    start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                    end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                    if start_index is not None and end_index is not None:\n",
    "                        for d in range(start_index, end_index + 1):\n",
    "                            new_split_text.append(f\"Điều {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ Điều\" in split_text_part:\n",
    "            start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if start_dieu_match:\n",
    "                start_dieu = start_dieu_match.group(1)\n",
    "        elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "            end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if end_dieu_match:\n",
    "                end_dieu = end_dieu_match.group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                if start_index is not None and end_index is not None:\n",
    "                    for d in range(start_index, end_index + 1):\n",
    "                        new_split_text.append(f\"Điều {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_dieu_ranges(text):\n",
    "    # Tìm khoảng \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    dieu_range_match = re.search(r'từ Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    if not dieu_range_match:\n",
    "        return text\n",
    "\n",
    "    dieu_range_text = dieu_range_match.group(0)\n",
    "    dieu_list = split_and_transform_dieu_ranges(dieu_range_text)\n",
    "\n",
    "    # Tạo chuỗi mới từ danh sách các Điều\n",
    "    dieu_list_str = ', '.join(dieu_list)\n",
    "\n",
    "    # Thay thế đoạn văn bản \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\" bằng danh sách các Điều\n",
    "    new_text = text.replace(dieu_range_text, dieu_list_str)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_khoan_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_khoan = None\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ khoản\" in split_text_part.lower():\n",
    "                start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "                end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"khoản {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ khoản\" in split_text_part.lower():\n",
    "            start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "            end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"khoản {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_khoan_ranges(text):\n",
    "    # Tìm khoảng \"từ khoản X đến khoản Y\" hoặc \"từ khoản X tới khoản Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    khoan_range_matches = re.finditer(r'từ khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "\n",
    "    new_text = text\n",
    "    offset = 0\n",
    "\n",
    "    for match in khoan_range_matches:\n",
    "        khoan_range_text = match.group(0)\n",
    "        khoan_list = split_and_transform_khoan_ranges(khoan_range_text)\n",
    "\n",
    "        # Tạo chuỗi mới từ danh sách các khoản\n",
    "        khoan_list_str = ', '.join(khoan_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + khoan_list_str + new_text[end:]\n",
    "\n",
    "        # Cập nhật offset\n",
    "        offset += len(khoan_list_str) - len(khoan_range_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_diem_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "\n",
    "    diem_positions = [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_diem = None\n",
    "\n",
    "    for i, (start, end) in enumerate(diem_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ điểm\" in split_text_part.lower():\n",
    "                start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "                end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"điểm {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ điểm\" in split_text_part.lower():\n",
    "            start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "            end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"điểm {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_diem_ranges(text):\n",
    "    # Tìm khoảng \"từ điểm X đến điểm Y\" hoặc \"từ điểm X tới điểm Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    diem_range_matches = re.finditer(r'từ điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "\n",
    "    new_text = text\n",
    "    offset = 0\n",
    "\n",
    "    for match in diem_range_matches:\n",
    "        diem_range_text = match.group(0)\n",
    "        diem_list = split_and_transform_diem_ranges(diem_range_text)\n",
    "\n",
    "        # Tạo chuỗi mới từ danh sách các điểm\n",
    "        diem_list_str = ', '.join(diem_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + diem_list_str + new_text[end:]\n",
    "\n",
    "        # Cập nhật offset\n",
    "        offset += len(diem_list_str) - len(diem_range_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## dạng ..... 1, 2, 3 và .....,......\n",
    "def processing_ccpl_full(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "    check = replace_dieu_ranges(replace_khoan_ranges(replace_diem_ranges(text)))\n",
    "    check2 = replace_dieu_in_text(replace_khoan_in_text(replace_diem_in_text(check)))\n",
    "\n",
    "    return check2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ điều này tới điều kia\n",
    "\n",
    "\n",
    "def split_by_dieu(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "\n",
    "    text = ' ' + text\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ khoản này tới khoản kia\n",
    "\n",
    "def split_by_khoan(text):\n",
    "    if text is None:\n",
    "            return[text]\n",
    "\n",
    "    text = ' ' + text\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    first_khoan_start = khoan_positions[0][0]\n",
    "    if first_khoan_start > 0:\n",
    "        split_text.append(text[:first_khoan_start].strip())\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trích xuất điều\n",
    "def extract_dieu(text):\n",
    "    # Find individual \"Điều\" references\n",
    "    dieu_list = re.findall(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', text, re.IGNORECASE)\n",
    "\n",
    "    # Find joined \"Điều\" lists\n",
    "    joined_dieu_list = re.findall(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)\n",
    "\n",
    "    # Process joined \"Điều\" lists\n",
    "    for item in joined_dieu_list:\n",
    "        # Find all \"Điều\" references in the joined list\n",
    "        joined_dieu_refs = re.findall(r'\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', item)\n",
    "        # Add only the unique \"Điều\" references to the main list\n",
    "        for ref in joined_dieu_refs:\n",
    "            if ref not in dieu_list:\n",
    "                dieu_list.append(ref)\n",
    "    return dieu_list\n",
    "\n",
    "# Hàm trích xuất khoản\n",
    "def extract_khoan(text):\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_khoan_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if '-' in part:\n",
    "                start, end = part.split('-')\n",
    "                for idx in range(int(start), int(end) + 1):\n",
    "                    final_khoan_list.append(str(idx))\n",
    "            elif len(part) == 1 and part.isalpha() or part[0].isdigit():\n",
    "                final_khoan_list.append(part)\n",
    "    return final_khoan_list\n",
    "\n",
    "# Hàm trích xuất điểm\n",
    "def extract_diem(text):\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_diem_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if re.match(r'^[a-zA-ZđĐ]\\d*(\\.\\d+)*$', part) or re.match(r'^\\d+(\\.\\d+)*$', part):\n",
    "                final_diem_list.append(part)\n",
    "    return final_diem_list\n",
    "\n",
    "# hàm xử lý đầu ra ccpl\n",
    "def processing_output_ccpl(input_data):\n",
    "    output_array = []\n",
    "    for dieu in input_data:\n",
    "        dieu_value = \", \".join(dieu[\"Dieu\"])\n",
    "        if dieu[\"Khoan\"]:\n",
    "            for khoan in dieu[\"Khoan\"]:\n",
    "                for khoan_item in khoan[\"Khoan\"]:\n",
    "                    if khoan[\"Diem\"]:\n",
    "                        for diem in khoan[\"Diem\"]:\n",
    "                            for diem_item in diem:\n",
    "                                output_array.extend([\n",
    "                                    dieu_value,\n",
    "                                    khoan_item,\n",
    "                                    diem_item\n",
    "                                ])\n",
    "                    else:\n",
    "                        output_array.extend([\n",
    "                            dieu_value,\n",
    "                            khoan_item,\n",
    "                            \"0\"\n",
    "                        ])\n",
    "        else:\n",
    "            output_array.extend([\n",
    "                dieu_value,\n",
    "                \"0\",\n",
    "                \"0\"\n",
    "            ])\n",
    "    if output_array:\n",
    "        return output_array\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to remove duplicate ccpl entries\n",
    "def remove_duplicates_ccpls(data):\n",
    "    for item in data:\n",
    "        seen = set()\n",
    "        unique_ccpls = []\n",
    "        for ccpl in item['ccpls']:\n",
    "            if ccpl['Dieu']:  # Kiểm tra nếu 'Dieu' không phải là rỗng\n",
    "                ccpl_tuple = (ccpl['LawID'], ccpl['LawTitle'], ccpl['Dieu'], ccpl['Khoan'], ccpl['Diem'])\n",
    "                if ccpl_tuple not in seen:\n",
    "                    seen.add(ccpl_tuple)\n",
    "                    unique_ccpls.append(ccpl)\n",
    "        item['ccpls'] = unique_ccpls\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_link_p_tag(data):\n",
    "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
    "    # Tập hợp để kiểm tra các liên kết đã thấy\n",
    "    link = []\n",
    "    # Lấy tất cả các thẻ <h2>\n",
    "    # Tập hợp để lưu các thẻ table đã được xử lý\n",
    "    processed_tables = set()\n",
    "    # Tạo danh sách temp_results mới cho mỗi thẻ <h2>\n",
    "    temp_results = []\n",
    "\n",
    "    # Tìm tất cả các thẻ giữa h2_tag và next_h2_tag\n",
    "    for element in soup:\n",
    "        if element.name == 'p' and element not in processed_tables:\n",
    "            processed_tables.add(element)\n",
    "            # for table_tag1 in element.find_all(\"tbody\"):\n",
    "            # for tr_tag in element.find_all('span'):\n",
    "            for a_tag in element.find_all('a'):\n",
    "                href = a_tag.get('href')\n",
    "                if href:\n",
    "                    links = {}\n",
    "                    href = href.strip()\n",
    "                    if not any(link['url'] == href for link in temp_results) and href.endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')):\n",
    "                        links['title'] = element.text.strip() #a_tag\n",
    "                        links['url'] = href\n",
    "                        temp_results.append(links)\n",
    "        elif element.name == 'table' and element not in processed_tables:\n",
    "            processed_tables.add(element)\n",
    "            for table_tag1 in element.find_all(\"tr\"):\n",
    "                for td_tag in table_tag1.find_all('td'):\n",
    "                    for a_tag in td_tag.find_all('a'):\n",
    "                        href = a_tag.get('href')\n",
    "                        if href:\n",
    "                            links = {}\n",
    "                            href = href.strip()\n",
    "                            if not any(link['url'] == href for link in temp_results) and href.endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')):\n",
    "                                links['title'] = table_tag1.text.strip()\n",
    "                                links['url'] = href\n",
    "                                temp_results.append(links)\n",
    "        link.extend(temp_results)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_ccpls(data_new):\n",
    "    soup = BeautifulSoup(data_new, 'html.parser', from_encoding= 'utf-8')\n",
    "    ccpls = []\n",
    "    for h2_tag in soup:\n",
    "        temp_results = []\n",
    "        next_tag = h2_tag.find_next_sibling()\n",
    "        gay_flag = True\n",
    "        while next_tag:\n",
    "            p_tag = next_tag.text\n",
    "            if (re.search(r\"thư\\s+viện\\s+pháp\\s+luật\", next_tag.text.lower())):\n",
    "                gay_flag = False\n",
    "            if next_tag.find('a') and not next_tag.text.startswith('>>') and gay_flag == True:\n",
    "                for a_tag in next_tag.find_all('a'):\n",
    "                    href = a_tag.get('href')\n",
    "                    if href and '.aspx' in href:\n",
    "                        final = processing_ccpl_full(a_tag.text)\n",
    "                        positions = split_by_dieu(final)\n",
    "                        # test = p_tag.split(positions)\n",
    "                        # print(positions)\n",
    "                        lawtitle = positions[-1]\n",
    "                        test = p_tag.split(lawtitle)\n",
    "                        # print(lawtitle)\n",
    "                        anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                        anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                        if \"điều\" in test[0].lower():\n",
    "                            p_tag = p_tag.replace(test[0], '', 1)\n",
    "                            final = processing_ccpl_full(test[0])\n",
    "                            positions = split_by_dieu(final)\n",
    "                            data_ccpl = []\n",
    "                            for dieu in positions:\n",
    "                                    if \"điều\" in dieu.lower():\n",
    "                                        dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                        # Tìm anchor trong href\n",
    "                                        khoan_list = split_by_khoan(dieu)\n",
    "                                        for khoan in khoan_list:\n",
    "                                            if \"khoản\" in khoan.lower():\n",
    "                                                khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                if \"điểm\" in khoan.lower():\n",
    "                                                    diem_dict = extract_diem(khoan)\n",
    "                                                    khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                        data_ccpl.append(dieu_dict)\n",
    "                                        # print(data_ccpl)\n",
    "                                        # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                        ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                        if len(ccpl_temp) > 3:\n",
    "                                            for i in range(0, len(ccpl_temp), 3):\n",
    "                                                # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                chunk = ccpl_temp[i:i+3]\n",
    "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                # print(ccpl_temp)\n",
    "                                                if len(chunk) > 0:\n",
    "                                                    dictionary = {\n",
    "                                                        \"url\": href,\n",
    "                                                        \"anchor\": anchor,\n",
    "                                                        \"LawID\": law_id,\n",
    "                                                        \"LawTitle\": lawtitle,\n",
    "                                                        \"Dieu\": chunk[0],\n",
    "                                                        \"Khoan\": chunk[1],\n",
    "                                                        \"Diem\": chunk[2]\n",
    "                                                    }\n",
    "                                                    temp_results.append(dictionary)\n",
    "                                        else:\n",
    "                                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                            law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                            law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                            if len(ccpl_temp) > 0:\n",
    "                                                dictionary = {\n",
    "                                                    \"url\": href,\n",
    "                                                    \"anchor\": anchor,\n",
    "                                                    \"LawID\": law_id,\n",
    "                                                    \"LawTitle\": lawtitle,\n",
    "                                                    \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                    \"Khoan\": ccpl_temp[1],\n",
    "                                                    \"Diem\": ccpl_temp[2]\n",
    "                                                }\n",
    "                                                temp_results.append(dictionary)\n",
    "\n",
    "                        else:\n",
    "                            temp_else = {}\n",
    "                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                            law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                            law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                            # law_title = a_tag.get_text()\n",
    "                            temp_else[\"url\"] = href\n",
    "                            temp_else[\"anchor\"] = anchor\n",
    "                            temp_else[\"LawID\"] = law_id\n",
    "                            temp_else[\"LawTitle\"] = lawtitle\n",
    "                            temp_else[\"Dieu\"] = \"0\"\n",
    "                            temp_else[\"Khoan\"] = \"0\"\n",
    "                            temp_else[\"Diem\"] = \"0\"\n",
    "                            temp_results.append(temp_else)\n",
    "                gay_flag = True\n",
    "            next_tag = next_tag.find_next_sibling()\n",
    "    # ccpls.extend(temp_results)\n",
    "        ccpls.extend(temp_results)\n",
    "    return ccpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def post_request(url, headers):\n",
    "    response = requests.post(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def post_request_key(url, headers, data):\n",
    "    try:\n",
    "        data = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        data.raise_for_status()  # Raise an error for bad status codes\n",
    "        return data.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng hàm để thực hiện yêu cầu POST\n",
    "def get_api():\n",
    "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "    headers1 = {\n",
    "        'Cookie': 'Culture=vi; Culture=vi'\n",
    "    }\n",
    "\n",
    "    key = post_request(url1, headers1)\n",
    "\n",
    "    url = 'https://apids.thuvienphapluat.vn/data/get-chinhsachphapluat'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "        'Content-Type': 'application/json',\n",
    "        'Cookie': 'Culture=vi'\n",
    "    }\n",
    "    return url, headers\n",
    "\n",
    "def send_data_to_api(data):\n",
    "    url, headers = get_api()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_new):\n",
    "    soup = BeautifulSoup(data_new['content'], 'html.parser', from_encoding= 'utf-8')\n",
    "        # xóa mục trích internet\n",
    "    # Tìm tất cả các thẻ <p> chứa từ \"từ internet)\"\n",
    "    p_tags_with_tu_internet = soup.find_all('p', string=lambda text: text and 'từ internet)' in text.lower())\n",
    "    # Xóa các thẻ <p> tìm được\n",
    "    for p_tag in p_tags_with_tu_internet:\n",
    "        # print(p_tag.text)\n",
    "        p_tag.decompose()\n",
    "\n",
    "    # # xóa mục trích internet\n",
    "    for table_tag in soup.find_all('p', align=\"center\"):\n",
    "        if not table_tag.find_parent('td', style=\"height:48px;\"):\n",
    "            # Tìm tất cả các thẻ <em> bên trong mỗi thẻ <p>\n",
    "            for table_tag1 in table_tag.find_all('em'):\n",
    "                # In ra nội dung của thẻ <em> dưới dạng chữ thường\n",
    "                # print(table_tag1.text.lower())\n",
    "                # Loại bỏ thẻ <em> khỏi cấu trúc cây HTML\n",
    "                table_tag1.decompose()\n",
    "                \n",
    "    # # xóa mục trích internet\n",
    "    for table_tag in soup.find_all('p', align=\"center\", recursive=False):\n",
    "        # print(table_tag.text)\n",
    "        table_tag.decompose()\n",
    "\n",
    "    # xóa mục trích internet\n",
    "    # Tìm tất cả các thẻ <p> có style \"text-align: center;\"\n",
    "    for table_tag in soup.find_all('p', style=\"text-align: center;\"):\n",
    "        # Kiểm tra nếu thẻ <p> không chứa thẻ <br>\n",
    "        if not table_tag.find('br'):\n",
    "            # Tìm tất cả các thẻ <em> bên trong mỗi thẻ <p>\n",
    "            for table_tag1 in table_tag.find_all('em'):\n",
    "                # In ra nội dung của thẻ <em> dưới dạng chữ thường\n",
    "                # print(table_tag1.text.lower())\n",
    "                # Loại bỏ thẻ <em> khỏi cấu trúc cây HTML\n",
    "                table_tag1.decompose()\n",
    "\n",
    "    # xóa tên tác giả\n",
    "    for table_tag in soup.find_all('p', align=\"right\"):\n",
    "        for table_tag1 in table_tag.find_all('strong', recursive=False):\n",
    "            # print(table_tag1.text)\n",
    "            table_tag1.decompose()\n",
    "# p>\\r\\n\\r\\n<p style=\\\"text-align: right;\\\"><strong>T&ocirc; Quốc Tr&igrave;nh</strong></p>\\r\\n\",\n",
    "    for table_tag in soup.find_all('p', \"text-align: right\"):\n",
    "        for table_tag1 in table_tag.find_all('strong', recursive=False):\n",
    "            # print(table_tag1.text)\n",
    "            table_tag1.decompose()\n",
    "\n",
    "#     for table_tag in soup.find_all('p'):\n",
    "#         if table_tag.text.lower().startswith('xem thêm'):\n",
    "#         # for table_tag1 in table_tag.find_all('strong', recursive=False):\n",
    "#             # print(table_tag1.text)\n",
    "#             table_tag.decompose()\n",
    "\n",
    "    # xóa mục tải về\n",
    "    # Tìm tất cả các thẻ <p> có thẻ <a> chứa từ 'tại đây'\n",
    "    p_tags_with_a_containing_tai_day = soup.find_all('p', string=lambda text: text and 'tại đây' in text)\n",
    "    # In ra các thẻ <p> tìm được\n",
    "    for p_tag in p_tags_with_a_containing_tai_day:\n",
    "        # print(p_tag.text)\n",
    "        p_tag.decompose()\n",
    "\n",
    "    # xóa title phần tải về\n",
    "    for table_tag in soup.find_all('table', border=\"0\", cellpadding=\"0\", cellspacing=\"0\"):\n",
    "        for table_tag1 in table_tag.find_all('tbody'):\n",
    "            for table_tag2 in table_tag1.find_all('tr'):\n",
    "                for table_tag3 in table_tag2.find_all(\"td\"):\n",
    "                    # , recursive=False\n",
    "                    if not table_tag3.find('p', align=\"center\"):\n",
    "                        if not table_tag3.find('em'):\n",
    "                            # print(table_tag3.text)\n",
    "                            table_tag3.decompose()\n",
    "\n",
    "    # xóa bảng trích thừa\n",
    "    # for table_tag in soup.find_all('table', border=\"1\", cellpadding=\"0\", cellspacing=\"0\"):\n",
    "    #     for table_tag1 in table_tag.find_all(\"td\", style=\"width: 623px; background-color: rgb(255, 204, 204);\"):\n",
    "    #         print(table_tag1.text)\n",
    "    #         table_tag1.decompose()\n",
    "        # table_tag1.decompose()\n",
    "\n",
    "    # xóa quảng cáo tải iThong\n",
    "    for p_tag in soup.find_all('p', style=lambda value: value and 'box-sizing: border-box' in value):\n",
    "        # print(p_tag.text)\n",
    "        p_tag.decompose()\n",
    "    text = soup.get_text()\n",
    "    temp_dict = {\n",
    "    \"question\": data_new['title'],\n",
    "    \"answer\": text,\n",
    "    \"links\": processing_link_p_tag(data_new['content']),\n",
    "    # + processing_link_p_tag(data_new[0]['content']),\n",
    "    \"ccpls\": processing_ccpls(data_new['content'])\n",
    "    }\n",
    "    data_final = []\n",
    "    data_final.append(temp_dict)\n",
    "    # Remove duplicates\n",
    "    questions_and_answers_final = remove_duplicates_ccpls(data_final)\n",
    "    return questions_and_answers_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"objid\": 63054, \"source\": \"CSPLM.ALL\", \"data\": [{\"question\": \"Đảm bảo công tác y tế trong dịp nghỉ lễ 30/4 và 01/5 năm 2024\", \"answer\": \"Đảm bảo công tác y tế trong dịp nghỉ lễ 30/4 và 01/5 năm 2024\\nTheo đó, nhằm đảm bảo công tác y tế và phòng, chống dịch trong các ngày nghỉ lễ 30/4 và 01/5, Bộ Y tế đã ban hành Công văn 2078/BYT-VPB1 năm 2024 trong đó yêu cầu các Cơ quan, Đơn vị thực hiện một số nội dung cụ thể như sau:\\n- Yêu cầu các cơ quan có liên quan tiếp tục thực hiện nghiêm các văn bản chỉ đạo của Chính phủ, Thủ tướng Chính phủ, Bộ Y tế, Ủy ban nhân dân các tỉnh, thành phố trực thuộc Trung ương trong công tác phòng, chống dịch bệnh, đảm bảo vệ sinh an toàn thực phẩm và thường trực khám bệnh, chữa bệnh;\\n+ Nâng cao tinh thần trách nhiệm, tuyệt đối không được chủ quan, lơ là, nêu cao ý thức và trách nhiệm của cá nhân, tập thể trong phạm vi chức năng, nhiệm vụ quyền hạn được giao.\\n- Bố trí phân công cán bộ trực 24/24 giờ trong những ngày nghỉ Lễ;\\n+ Các cơ quan liên quan cần đảm bảo các điều kiện về nhân lực, cơ sở vật chất, thuốc, trang thiết bị, vật tư y tế duy trì các hoạt động thường xuyên của các Cơ quan, Đơn vị;\\n+ Chủ động theo dõi sát, nắm tình hình và xử lý những vấn đề phát sinh về công tác y tế; đảm bảo thực hiện đầy đủ các chế độ cho người lao động theo quy định của pháp luật.\\n- Phối hợp chặt chẽ với lực lượng Công an và các Đơn vị có trách nhiệm liên quan trên địa bàn đảm bảo an ninh trật tự, an toàn giao thông, phòng chống tội phạm, phòng chống cháy nổ, công tác y tế và các vấn đề liên quan khác.\\n- Báo cáo kịp thời về những vấn đề nổi cộm, phức tạp phát sinh (nếu có) về công tác y tế cho Bộ Y tế thông qua Văn phòng Bộ Y tế.\\n- Bảng phân công trực Lãnh đạo Bộ Y tế gửi kèm theo Công văn 2078/BYT-VPB1 năm 2024.\\nXem thêm tại Công văn 2078/BYT-VPB1 được ban hành ngày 23/4/2024.\\n \\n\", \"links\": [], \"ccpls\": []}], \"type\": 1}\n"
     ]
    }
   ],
   "source": [
    "data_api = {\n",
    "    \"page\": 12410, #14039\n",
    "    \"num\": 1,\n",
    "    \"type\": \"tbvbm\"\n",
    "}\n",
    "url, headers = get_api()\n",
    "response = post_request_key(url, headers, data_api)\n",
    "\n",
    "if response:\n",
    "    data_input = response.get('data')\n",
    "    for temp in data_input:\n",
    "        extracted_data = extract_data(temp)\n",
    "        dictionary = {\n",
    "            \"objid\": int(temp['obj_id']),\n",
    "            \"source\": str(temp['obj_code']),\n",
    "            \"data\": extracted_data,\n",
    "            \"type\": 1\n",
    "        }\n",
    "        dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "        print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_note = []\n",
    "# # # Số trang cần lấy\n",
    "# specific_page =  12844 #46146 pldn ko có tbvbm\n",
    "# # print(now)\n",
    "# num = 100\n",
    "# total_page = specific_page // num + ( 1 if specific_page % num > 0 else 0)\n",
    "# for num_page in range(total_page, 0, -1):\n",
    "#     data_api = {\n",
    "#     \"page\": num_page,\n",
    "#     \"num\": num,\n",
    "#     \"type\": \"tbvbm\"\n",
    "#     }\n",
    "#     url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "#     headers1 = {\n",
    "#         'Cookie': 'Culture=vi; Culture=vi'\n",
    "#     }\n",
    "\n",
    "#     key = post_request(url1, headers1)\n",
    "#     url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
    "\n",
    "#     headers_post = {\n",
    "#     'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "#     'Content-Type': 'application/json',\n",
    "#     'Cookie': 'Culture=vi'\n",
    "#     }\n",
    "\n",
    "#     url, headers = get_api()\n",
    "#     response = post_request_key(url, headers, data_api)\n",
    "#     # print(response.get('data'))\n",
    "#     if response:\n",
    "#         data_input = response.get('data')\n",
    "#         for temp in data_input:\n",
    "#             # print(temp[\"content\"])\n",
    "#             extracted_data = extract_data(temp)\n",
    "#             # print(extract_data(temp[\"content\"]))\n",
    "#             if temp['obj_id'] in file_note_old:\n",
    "#                 #  extracted_data != [] and \n",
    "#                 #  and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE)\n",
    "#                 dictionary = {\n",
    "#                     \"objid\": int(temp['obj_id']),\n",
    "#                     \"source\": str(temp['obj_code']),\n",
    "#                     \"data\": extracted_data,\n",
    "#                     \"type\": 1\n",
    "#                 }\n",
    "#                 dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "#                 # print(dictionary)\n",
    "#                 response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "#                 # print(dictionary)\n",
    "#                 print(response.text)\n",
    "#                 # print(num_page)\n",
    "#             else:\n",
    "#                 file_note.append(int(temp['obj_id']))\n",
    "#                 # time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_note' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfile_note\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_note' is not defined"
     ]
    }
   ],
   "source": [
    "file_note"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Minh-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
