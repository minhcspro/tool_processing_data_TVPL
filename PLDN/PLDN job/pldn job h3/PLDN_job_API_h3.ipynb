{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFumuaqiNX7X"
      },
      "source": [
        "import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g-wzutHqNX7b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup, NavigableString,Tag\n",
        "from tabulate import tabulate\n",
        "import unicodedata\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beyiu2GzNX7c"
      },
      "source": [
        "Function processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "############ Điều 1, 2, 3 + replace\n",
        "import re\n",
        "\n",
        "def split_by_dieu_new1(text):\n",
        "    # def extract_dieu_positions(text):\n",
        "    if text is None:\n",
        "        return[text]\n",
        "\n",
        "        \n",
        "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)]\n",
        "    if not dieu_positions:\n",
        "        return [text]\n",
        "\n",
        "    split_text = []\n",
        "    current_position = 0\n",
        "\n",
        "    for start, end in dieu_positions:\n",
        "        if current_position < start:\n",
        "            split_text.append(text[current_position:start].strip())\n",
        "        split_text.append(text[start:end].strip())\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text.append(text[current_position:].strip())\n",
        "\n",
        "    return split_text\n",
        "################\n",
        "def extract_dieu_err(text):\n",
        "    # Define the pattern to match \"Điều\" followed by numbers and possibly letters or other characters\n",
        "    pattern = r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?'\n",
        "\n",
        "    # Find all matches of the pattern in the text\n",
        "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
        "    matched_phrases = [match.group(0) for match in matches]\n",
        "    check_phrases = []\n",
        "\n",
        "    # Loop through each phrase in matched_phrases\n",
        "    for phrase in matched_phrases:\n",
        "        if ',' in phrase or ' và ' in phrase:\n",
        "            check_phrases.append(phrase)\n",
        "    return check_phrases\n",
        "##############\n",
        "def process_dieu_parts(text):\n",
        "    dieu_final = []\n",
        "    for match in text:\n",
        "        if ',' in match or ' và ' in match:\n",
        "            splitted = re.split(r',| và ', match)\n",
        "            for temp in splitted:\n",
        "                if re.match(r'(?i)Điều\\s+\\d+', temp):\n",
        "                    dieu_final.append(temp)\n",
        "                else:\n",
        "                    dieu_final.append('Điều ' + temp)\n",
        "\n",
        "    return dieu_final\n",
        "###############\n",
        "def replace_dieu_in_text(text):\n",
        "    if text is None:\n",
        "        return[text]\n",
        "    split_text = split_by_dieu_new1(text)\n",
        "    matches = extract_dieu_err(text)\n",
        "    if not matches:\n",
        "        return text\n",
        "    dieu_final = process_dieu_parts(matches)\n",
        "    replaced_text = []\n",
        "    for part in split_text:\n",
        "        if re.match(r'(?i)Điều\\s+\\d+', part) and \" và \" in part:\n",
        "            for dieu in dieu_final:\n",
        "                replaced_text.append(dieu)\n",
        "        else:\n",
        "            replaced_text.append(part)\n",
        "\n",
        "    return ' '.join(replaced_text)\n",
        "\n",
        "#############\n",
        "#############\n",
        "########### khoản 1, 2,3  và .... + replace\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def split_by_khoan_new1(text):\n",
        "    if text is None:\n",
        "        return[text]       \n",
        "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
        "    if not khoan_positions:\n",
        "        return [text]\n",
        "\n",
        "    split_text = []\n",
        "    current_position = 0\n",
        "\n",
        "    for start, end in khoan_positions:\n",
        "        if current_position < start:\n",
        "            split_text.append(text[current_position:start].strip())\n",
        "        split_text.append(text[start:end].strip())\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text.append(text[current_position:].strip())\n",
        "\n",
        "    return split_text\n",
        "\n",
        "#####################\n",
        "def extract_khoan_err(text):\n",
        "    # Define the pattern to match \"khoản\" followed by numbers and possibly letters or other characters\n",
        "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
        "\n",
        "    # Find all matches of the pattern in the text\n",
        "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
        "    matched_phrases = [match.group(0) for match in matches]\n",
        "    check_phrases = []\n",
        "\n",
        "    # Loop through each phrase in matched_phrases\n",
        "    for phrase in matched_phrases:\n",
        "        if ',' in phrase or ' và ' in phrase:\n",
        "            check_phrases.append(phrase)\n",
        "    return check_phrases\n",
        "################\n",
        "def process_khoan_parts(text):\n",
        "    khoan_final = []\n",
        "    for match in text:\n",
        "        if ',' in match or ' và ' in match:\n",
        "            splitted = re.split(r',| và ', match)\n",
        "            for temp in splitted:\n",
        "                if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
        "                    khoan_final.append(temp)\n",
        "                else:\n",
        "                    khoan_final.append('khoản ' + temp)\n",
        "\n",
        "    return khoan_final\n",
        "###############\n",
        "\n",
        "def replace_khoan_in_text(text):\n",
        "    if text is None:\n",
        "        return[text]\n",
        "    split_text = split_by_khoan_new1(text)\n",
        "    matches = extract_khoan_err(text)\n",
        "    if not matches:\n",
        "        return text\n",
        "    khoan_final = process_khoan_parts(matches)\n",
        "    replaced_text = []\n",
        "    for part in split_text:\n",
        "        if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
        "            for khoan in khoan_final:\n",
        "                replaced_text.append(khoan)\n",
        "        else:\n",
        "            replaced_text.append(part)\n",
        "\n",
        "    return ' '.join(replaced_text)\n",
        "#############\n",
        "#############\n",
        "################# Điểm 1,2,3 và .... + replace\n",
        "import re\n",
        "\n",
        "def split_by_diem_new1(text):\n",
        "    def extract_diem_positions(text):\n",
        "        return [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])', text, re.IGNORECASE)]\n",
        "        \n",
        "    diem_positions = extract_diem_positions(text)\n",
        "    if not diem_positions:\n",
        "        return [text]\n",
        "\n",
        "    split_text = []\n",
        "    current_position = 0\n",
        "\n",
        "    for start, end in diem_positions:\n",
        "        if current_position < start:\n",
        "            split_text.append(text[current_position:start].strip())\n",
        "        split_text.append(text[start:end].strip())\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text.append(text[current_position:].strip())\n",
        "\n",
        "    return split_text\n",
        "###########################\n",
        "def extract_diem_err(text):\n",
        "    # Define the pattern to match \"điểm\" followed by numbers and possibly letters or other characters\n",
        "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
        "\n",
        "    # Find all matches of the pattern in the text\n",
        "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
        "    matched_phrases = [match.group(0) for match in matches]\n",
        "    check_phrases = []\n",
        "\n",
        "    # Loop through each phrase in matched_phrases\n",
        "    for phrase in matched_phrases:\n",
        "        if ',' in phrase or ' và ' in phrase:\n",
        "            check_phrases.append(phrase)\n",
        "    return check_phrases\n",
        "#########################\n",
        "def process_diem_parts(text):\n",
        "    diem_final = []\n",
        "    for match in text:\n",
        "        if ',' in match or ' và ' in match:\n",
        "            splitted = re.split(r',| và ', match)\n",
        "            for temp in splitted:\n",
        "                if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
        "                    diem_final.append(temp)\n",
        "                else:\n",
        "                    diem_final.append('điểm ' + temp)\n",
        "\n",
        "    return diem_final\n",
        "#########################\n",
        "def replace_diem_in_text(text):\n",
        "    if text is None:\n",
        "        return[text]\n",
        "    split_text = split_by_diem_new1(text)\n",
        "    matches = extract_diem_err(text)\n",
        "    if not matches:\n",
        "        return text\n",
        "    diem_final = process_diem_parts(matches)\n",
        "    replaced_text = []\n",
        "    for part in split_text:\n",
        "        if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
        "            for diem in diem_final:\n",
        "                replaced_text.append(diem)\n",
        "        else:\n",
        "            replaced_text.append(part)\n",
        "\n",
        "    return ' '.join(replaced_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_and_transform_dieu_ranges(text):\n",
        "    if text is None:\n",
        "        return [text]\n",
        "\n",
        "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
        "\n",
        "    if not dieu_positions:\n",
        "        return [text]\n",
        "\n",
        "    new_split_text = []\n",
        "    current_position = 0\n",
        "    start_dieu = None\n",
        "\n",
        "    for i, (start, end) in enumerate(dieu_positions):\n",
        "        if current_position < start:\n",
        "            split_text_part = text[current_position:end].strip()\n",
        "            if \"từ Điều\" in split_text_part:\n",
        "                start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
        "                if start_dieu_match:\n",
        "                    start_dieu = start_dieu_match.group(1)\n",
        "            elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
        "                end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
        "                if end_dieu_match:\n",
        "                    end_dieu = end_dieu_match.group(1)\n",
        "                    start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
        "                    end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
        "                    if start_index is not None and end_index is not None:\n",
        "                        for d in range(start_index, end_index + 1):\n",
        "                            new_split_text.append(f\"Điều {d}\")\n",
        "            else:\n",
        "                new_split_text.append(split_text_part)\n",
        "\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text_part = text[current_position:].strip()\n",
        "        if \"từ Điều\" in split_text_part:\n",
        "            start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
        "            if start_dieu_match:\n",
        "                start_dieu = start_dieu_match.group(1)\n",
        "        elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
        "            end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
        "            if end_dieu_match:\n",
        "                end_dieu = end_dieu_match.group(1)\n",
        "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
        "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
        "                if start_index is not None and end_index is not None:\n",
        "                    for d in range(start_index, end_index + 1):\n",
        "                        new_split_text.append(f\"Điều {d}\")\n",
        "        else:\n",
        "            new_split_text.append(split_text_part)\n",
        "\n",
        "    return new_split_text\n",
        "\n",
        "def replace_dieu_ranges(text):\n",
        "    # Tìm khoảng \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\"\n",
        "    if text is None:\n",
        "        return[text]\n",
        "    dieu_range_match = re.search(r'từ Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
        "    if not dieu_range_match:\n",
        "        return text\n",
        "\n",
        "    dieu_range_text = dieu_range_match.group(0)\n",
        "    dieu_list = split_and_transform_dieu_ranges(dieu_range_text)\n",
        "    \n",
        "    # Tạo chuỗi mới từ danh sách các Điều\n",
        "    dieu_list_str = ', '.join(dieu_list)\n",
        "\n",
        "    # Thay thế đoạn văn bản \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\" bằng danh sách các Điều\n",
        "    new_text = text.replace(dieu_range_text, dieu_list_str)\n",
        "\n",
        "    return new_text\n",
        "\n",
        "def split_and_transform_khoan_ranges(text):\n",
        "    if text is None:\n",
        "        return [text]\n",
        "        \n",
        "\n",
        "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
        "    if not khoan_positions:\n",
        "        return [text]\n",
        "\n",
        "    new_split_text = []\n",
        "    current_position = 0\n",
        "    start_khoan = None\n",
        "\n",
        "    for i, (start, end) in enumerate(khoan_positions):\n",
        "        if current_position < start:\n",
        "            split_text_part = text[current_position:end].strip()\n",
        "            if \"từ khoản\" in split_text_part.lower():\n",
        "                start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "            elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
        "                end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
        "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
        "                for d in range(start_index, end_index + 1):\n",
        "                    new_split_text.append(f\"khoản {d}\")\n",
        "            else:\n",
        "                new_split_text.append(split_text_part)\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text_part = text[current_position:].strip()\n",
        "        if \"từ khoản\" in split_text_part.lower():\n",
        "            start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "        elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
        "            end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
        "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
        "            for d in range(start_index, end_index + 1):\n",
        "                new_split_text.append(f\"khoản {d}\")\n",
        "        else:\n",
        "            new_split_text.append(split_text_part)\n",
        "\n",
        "    return new_split_text\n",
        "\n",
        "def replace_khoan_ranges(text):\n",
        "    # Tìm khoảng \"từ khoản X đến khoản Y\" hoặc \"từ khoản X tới khoản Y\"\n",
        "    if text is None:\n",
        "        return[text]\n",
        "    khoan_range_matches = re.finditer(r'từ khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
        "    \n",
        "    new_text = text\n",
        "    offset = 0\n",
        "    \n",
        "    for match in khoan_range_matches:\n",
        "        khoan_range_text = match.group(0)\n",
        "        khoan_list = split_and_transform_khoan_ranges(khoan_range_text)\n",
        "        \n",
        "        # Tạo chuỗi mới từ danh sách các khoản\n",
        "        khoan_list_str = ', '.join(khoan_list)\n",
        "\n",
        "        # Tính toán vị trí mới sau khi thay thế\n",
        "        start, end = match.start() + offset, match.end() + offset\n",
        "        new_text = new_text[:start] + khoan_list_str + new_text[end:]\n",
        "        \n",
        "        # Cập nhật offset\n",
        "        offset += len(khoan_list_str) - len(khoan_range_text)\n",
        "\n",
        "    return new_text\n",
        "\n",
        "def split_and_transform_diem_ranges(text):\n",
        "    if text is None:\n",
        "        return [text]\n",
        "        \n",
        "\n",
        "    diem_positions = [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
        "    if not diem_positions:\n",
        "        return [text]\n",
        "\n",
        "    new_split_text = []\n",
        "    current_position = 0\n",
        "    start_diem = None\n",
        "\n",
        "    for i, (start, end) in enumerate(diem_positions):\n",
        "        if current_position < start:\n",
        "            split_text_part = text[current_position:end].strip()\n",
        "            if \"từ điểm\" in split_text_part.lower():\n",
        "                start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "            elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
        "                end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
        "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
        "                for d in range(start_index, end_index + 1):\n",
        "                    new_split_text.append(f\"điểm {d}\")\n",
        "            else:\n",
        "                new_split_text.append(split_text_part)\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text_part = text[current_position:].strip()\n",
        "        if \"từ điểm\" in split_text_part.lower():\n",
        "            start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "        elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
        "            end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
        "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
        "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
        "            for d in range(start_index, end_index + 1):\n",
        "                new_split_text.append(f\"điểm {d}\")\n",
        "        else:\n",
        "            new_split_text.append(split_text_part)\n",
        "\n",
        "    return new_split_text\n",
        "\n",
        "def replace_diem_ranges(text):\n",
        "    # Tìm khoảng \"từ điểm X đến điểm Y\" hoặc \"từ điểm X tới điểm Y\"\n",
        "    if text is None:\n",
        "        return[text]\n",
        "    diem_range_matches = re.finditer(r'từ điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
        "    \n",
        "    new_text = text\n",
        "    offset = 0\n",
        "    \n",
        "    for match in diem_range_matches:\n",
        "        diem_range_text = match.group(0)\n",
        "        diem_list = split_and_transform_diem_ranges(diem_range_text)\n",
        "        \n",
        "        # Tạo chuỗi mới từ danh sách các điểm\n",
        "        diem_list_str = ', '.join(diem_list)\n",
        "\n",
        "        # Tính toán vị trí mới sau khi thay thế\n",
        "        start, end = match.start() + offset, match.end() + offset\n",
        "        new_text = new_text[:start] + diem_list_str + new_text[end:]\n",
        "        \n",
        "        # Cập nhật offset\n",
        "        offset += len(diem_list_str) - len(diem_range_text)\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "########### Cắt từ điều này tới điều kia\n",
        "\n",
        "\n",
        "def split_by_dieu(text):\n",
        "    if text is None: \n",
        "        return[text]\n",
        "    \n",
        "    text = ' ' + text\n",
        "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
        "    if not dieu_positions:\n",
        "        return [text]\n",
        "\n",
        "    split_text = []\n",
        "    current_position = 0\n",
        "    \n",
        "    for i, (start, end) in enumerate(dieu_positions):\n",
        "        if current_position < start:\n",
        "            split_text.append(text[current_position:end].strip())\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text.append(text[current_position:].strip())\n",
        "\n",
        "    return split_text\n",
        "\n",
        "\n",
        "# text = \"Điều 12 luật adfwf dasdass và khoản 2 Điều 191, khoản 1231231 điều 11\"\n",
        "\n",
        "# a = split_by_dieu(result)\n",
        "# a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "########### Cắt từ khoản này tới khoản kia\n",
        "\n",
        "def split_by_khoan(text):\n",
        "    if text is None:\n",
        "            return[text]\n",
        "\n",
        "    text = ' ' + text\n",
        "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
        "    if not khoan_positions:\n",
        "        return [text]\n",
        "\n",
        "    split_text = []\n",
        "    current_position = 0\n",
        "\n",
        "    first_khoan_start = khoan_positions[0][0]\n",
        "    if first_khoan_start > 0:\n",
        "        split_text.append(text[:first_khoan_start].strip())\n",
        "\n",
        "    for i, (start, end) in enumerate(khoan_positions):\n",
        "        if current_position < start:\n",
        "            split_text.append(text[current_position:end].strip())\n",
        "        current_position = end\n",
        "\n",
        "    if current_position < len(text):\n",
        "        split_text.append(text[current_position:].strip())\n",
        "\n",
        "    return split_text\n",
        "\n",
        "\n",
        "# text = \"khoản 121 luật adfwf dasdass và khoản 2 Điều 191, khoản 1231231 điều 11\"\n",
        "# a = split_by_khoan(result)\n",
        "# a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "############## dạng ..... 1, 2, 3 và .....,......\n",
        "def processing_ccpl_full(text):\n",
        "    if text is None:\n",
        "        return [text]  \n",
        "    check = replace_dieu_ranges(replace_khoan_ranges(replace_diem_ranges(text)))\n",
        "    check2 = replace_dieu_in_text(replace_khoan_in_text(replace_diem_in_text(check)))\n",
        "\n",
        "    return check2\n",
        "\n",
        "# text = \"căn cứ từ Điều 3 đến Điều 7, Điều 1, 2, 3 và 4\"\n",
        "\n",
        "# # Kết quả sau khi thay thế\n",
        "# result = processing_ccpl_full(text)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vcC9vtYaNX7c"
      },
      "outputs": [],
      "source": [
        "# Hàm trích xuất điều\n",
        "def extract_dieu(text):\n",
        "    # Find individual \"Điều\" references\n",
        "    dieu_list = re.findall(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', text, re.IGNORECASE)\n",
        "\n",
        "    # Find joined \"Điều\" lists\n",
        "    joined_dieu_list = re.findall(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)\n",
        "\n",
        "    # Process joined \"Điều\" lists\n",
        "    for item in joined_dieu_list:\n",
        "        # Find all \"Điều\" references in the joined list\n",
        "        joined_dieu_refs = re.findall(r'\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', item)\n",
        "        # Add only the unique \"Điều\" references to the main list\n",
        "        for ref in joined_dieu_refs:\n",
        "            if ref not in dieu_list:\n",
        "                dieu_list.append(ref)\n",
        "    return dieu_list\n",
        "\n",
        "# Hàm trích xuất khoản\n",
        "def extract_khoan(text):\n",
        "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
        "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "    final_khoan_list = []\n",
        "    for match in matches:\n",
        "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
        "        for part in parts:\n",
        "            part = part.strip().strip('.').lower()\n",
        "            if '-' in part:\n",
        "                start, end = part.split('-')\n",
        "                for idx in range(int(start), int(end) + 1):\n",
        "                    final_khoan_list.append(str(idx))\n",
        "            elif len(part) == 1 and part.isalpha() or part[0].isdigit():\n",
        "                final_khoan_list.append(part)\n",
        "    return final_khoan_list\n",
        "\n",
        "# Hàm trích xuất điểm\n",
        "def extract_diem(text):\n",
        "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
        "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "    final_diem_list = []\n",
        "    for match in matches:\n",
        "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
        "        for part in parts:\n",
        "            part = part.strip().strip('.').lower()\n",
        "            if re.match(r'^[a-zA-ZđĐ]\\d*(\\.\\d+)*$', part) or re.match(r'^\\d+(\\.\\d+)*$', part):\n",
        "                final_diem_list.append(part)\n",
        "    return final_diem_list\n",
        "\n",
        "# hàm xử lý đầu ra ccpl\n",
        "def processing_output_ccpl(input_data):\n",
        "    output_array = []\n",
        "    for dieu in input_data:\n",
        "        dieu_value = \", \".join(dieu[\"Dieu\"])\n",
        "        if dieu[\"Khoan\"]:\n",
        "            for khoan in dieu[\"Khoan\"]:\n",
        "                for khoan_item in khoan[\"Khoan\"]:\n",
        "                    if khoan[\"Diem\"]:\n",
        "                        for diem in khoan[\"Diem\"]:\n",
        "                            for diem_item in diem:\n",
        "                                output_array.extend([\n",
        "                                    dieu_value,\n",
        "                                    khoan_item,\n",
        "                                    diem_item\n",
        "                                ])\n",
        "                    else:\n",
        "                        output_array.extend([\n",
        "                            dieu_value,\n",
        "                            khoan_item,\n",
        "                            \"0\"\n",
        "                        ])\n",
        "        else:\n",
        "            output_array.extend([\n",
        "                dieu_value,\n",
        "                \"0\",\n",
        "                \"0\"\n",
        "            ])\n",
        "    if output_array:\n",
        "        return output_array\n",
        "\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Function to remove duplicate ccpl entries\n",
        "def remove_duplicates_ccpls(data):\n",
        "    for item in data:\n",
        "        seen = set()\n",
        "        unique_ccpls = []\n",
        "        for ccpl in item['ccpls']:\n",
        "            if ccpl['Dieu']:  # Kiểm tra nếu 'Dieu' không phải là rỗng\n",
        "                ccpl_tuple = (ccpl['LawID'], ccpl['LawTitle'], ccpl['Dieu'], ccpl['Khoan'], ccpl['Diem'])\n",
        "                if ccpl_tuple not in seen:\n",
        "                    seen.add(ccpl_tuple)\n",
        "                    unique_ccpls.append(ccpl)\n",
        "        item['ccpls'] = unique_ccpls\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lVE38kQ-NX7f"
      },
      "outputs": [],
      "source": [
        "def processing_link_table(data):\n",
        "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "    # Tập hợp để kiểm tra các liên kết đã thấy\n",
        "    link = []\n",
        "    # Lấy tất cả các thẻ <h2>\n",
        "    h2_tags = soup.find_all('h3')\n",
        "    h2_tags_real = soup.find_all('h2')\n",
        "    # Tập hợp để lưu các thẻ table đã được xử lý\n",
        "    processed_tables = set()\n",
        "    for i in range(len(h2_tags)):\n",
        "        h2_tag = h2_tags[i]\n",
        "\n",
        "        # Tạo danh sách temp_results mới cho mỗi thẻ <h2>\n",
        "        temp_results = []\n",
        "\n",
        "        # Tìm tất cả các thẻ giữa h2_tag và next_h2_tag\n",
        "        for element in h2_tag.find_all_next():\n",
        "            if element in h2_tags or element in h2_tags_real:\n",
        "                break\n",
        "            if element.name == 'table' and element not in processed_tables:\n",
        "                processed_tables.add(element)\n",
        "                # for table_tag1 in element.find_all(\"tbody\"):\n",
        "                for tr_tag in element.find_all('tr'):\n",
        "                    for a_tag in tr_tag.find_all('a'):\n",
        "                        href = a_tag.get('href')\n",
        "                        if href:\n",
        "                            links = {}\n",
        "                            href = href.strip()\n",
        "                            if not any(link['url'] == href for link in temp_results) and href.endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')):\n",
        "                                links['title'] = tr_tag.text.strip()\n",
        "                                links['url'] = href\n",
        "                                temp_results.append(links)\n",
        "\n",
        "        # Thêm kết quả cho mỗi thẻ <h2> vào danh sách ques\n",
        "        link.append({\n",
        "            # \"question\": h2_tag.text.strip(),\n",
        "            \"links\": temp_results\n",
        "        })\n",
        "\n",
        "    return json.dumps(link, ensure_ascii=False, indent=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RTFa1LtLNX7g"
      },
      "outputs": [],
      "source": [
        "def processing_link_p_tag(data):\n",
        "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "    # Tập hợp để kiểm tra các liên kết đã thấy\n",
        "    link = []\n",
        "    # Lấy tất cả các thẻ <h2>\n",
        "    h2_tags = soup.find_all('h3')\n",
        "    h2_tags_real = soup.find_all('h2')\n",
        "    # Tập hợp để lưu các thẻ table đã được xử lý\n",
        "    processed_tables = set()\n",
        "    for i in range(len(h2_tags)):\n",
        "        h2_tag = h2_tags[i]\n",
        "\n",
        "        # Tạo danh sách temp_results mới cho mỗi thẻ <h2>\n",
        "        temp_results = []\n",
        "\n",
        "        # Tìm tất cả các thẻ giữa h2_tag và next_h2_tag\n",
        "        for element in h2_tag.find_all_next():\n",
        "            if element in h2_tags or element in h2_tags_real:\n",
        "                break\n",
        "            if element.name == 'p' and element not in processed_tables:\n",
        "                processed_tables.add(element)\n",
        "                # for table_tag1 in element.find_all(\"tbody\"):\n",
        "                for tr_tag in element.find_all('span'):\n",
        "                    for a_tag in tr_tag.find_all('a'):\n",
        "                        href = a_tag.get('href')\n",
        "                        if href:\n",
        "                            links = {}\n",
        "                            href = href.strip()\n",
        "                            if not any(link['url'] == href for link in temp_results) and href.endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')):\n",
        "                                links['title'] = a_tag.text.strip()\n",
        "                                links['url'] = href\n",
        "                                temp_results.append(links)\n",
        "\n",
        "        # Thêm kết quả cho mỗi thẻ <h2> vào danh sách ques\n",
        "        link.append({\n",
        "            # \"question\": h2_tag.text.strip(),\n",
        "            \"links\": temp_results\n",
        "        })\n",
        "\n",
        "    return json.dumps(link, ensure_ascii=False, indent=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# format các dạng bảng\n",
        "def table_to_markdown(table):\n",
        "    if table is None:\n",
        "        return \"\"\n",
        "    rows = []\n",
        "    headers = []\n",
        "\n",
        "    for row in table.find_all('tr'):\n",
        "        row_data = []\n",
        "        for cell in row.find_all(['th', 'td']):\n",
        "            cell_text = cell.get_text(strip=True)\n",
        "            row_data.append(cell_text)\n",
        "        if not headers:\n",
        "            headers = row_data\n",
        "        else:\n",
        "            rows.append(row_data)\n",
        "\n",
        "    return tabulate(rows, headers, tablefmt='pipe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aRHeyQ8PNX7g"
      },
      "outputs": [],
      "source": [
        "def processing_1(data):\n",
        "    # Tìm tất cả các thẻ <h2>\n",
        "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "\n",
        "    # xử lý download\n",
        "    for table_tag in soup.find_all('table', border=\"0\", cellpadding=\"0\", cellspacing=\"0\"):\n",
        "        if 'style' not in table_tag.attrs:\n",
        "            # print(table_tag.text.lower())\n",
        "            table_tag.decompose()\n",
        "    # **********************************************************************************************\n",
        "    # xử lý trích điều thừa\n",
        "    for table_tag in soup.find_all('table'):\n",
        "        # Check if there is any <td> tag found and if its text starts with 'Điều'\n",
        "        if table_tag and table_tag.text.strip().startswith('Điều'):\n",
        "            # Print or do something with the table\n",
        "            # print(table_tag.text)\n",
        "            table_tag.decompose()\n",
        "\n",
        "    # for table_tag in soup.find_all('table', align=\"center\", border=\"1\", cellpadding=\"1\", cellspacing=\"1\"):\n",
        "    #     # print(table_tag.text.lower())\n",
        "    #     table_tag.decompose()\n",
        "    # **********************************************************************************************\n",
        "    for table_tag in soup.find_all('p', style=\"text-align:center\", recursive=False):\n",
        "        for table_tag_1 in table_tag.find_all('span', style=\"color:#000000\", recursive=False):\n",
        "                for table_tag_2 in table_tag_1.find_all('span', style=\"font-size:14px\", recursive=False):\n",
        "                    for table_tag_3 in table_tag_2.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\", recursive=False):\n",
        "                        if not table_tag_3.find_all(True):\n",
        "                            # print(table_tag_3.text.lower())\n",
        "                            table_tag_3.decompose()\n",
        "\n",
        "    for table_tag in soup.find_all('p', style=\"text-align:center\", recursive=False):\n",
        "        for table_tag_1 in table_tag.find_all('span', style=\"color:#000000; font-family:Arial,Helvetica,sans-serif\", recursive=False):\n",
        "                for table_tag_2 in table_tag_1.find_all('span', style=\"font-size:14px\", recursive=False):\n",
        "                    if not table_tag_2.find_all(True):\n",
        "                        # print(table_tag_2.text.lower())\n",
        "                        table_tag_2.decompose()\n",
        "\n",
        "    # Tìm tất cả các thẻ <p> với style=\"text-align:justify\"\n",
        "    for table_tag in soup.find_all('p'):\n",
        "        # Tìm tất cả các thẻ <span> với style=\"font-size:14px\" bên trong thẻ <p>\n",
        "        for table_tag_1 in table_tag.find_all('span', style=\"font-size:14px\"):\n",
        "            # Tìm tất cả các thẻ <span> với style=\"font-family:Arial,Helvetica,sans-serif\" bên trong thẻ <span> trước\n",
        "            for table_tag_2 in table_tag_1.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\"):\n",
        "                # Kiểm tra nếu thẻ <span> này chứa thẻ <img>\n",
        "                if table_tag_2.find('img'):\n",
        "                    # Tìm tất cả các thẻ <strong> bên trong thẻ <span> chứa thẻ <img>\n",
        "                    for table_tag_3 in table_tag_2.find_all('strong'):\n",
        "                        # Tìm tất cả các thẻ <a> bên trong thẻ <strong>\n",
        "                        for table_tag_4 in table_tag_3.find_all('a'):\n",
        "                            # In ra nội dung của thẻ <a> và xóa thẻ <a>\n",
        "                            # print(table_tag_4.text.lower())\n",
        "                            table_tag_4.decompose()\n",
        "\n",
        "    # Tìm tất cả các thẻ <p> với style=\"text-align:justify\"\n",
        "    for table_tag in soup.find_all('table', style= None):\n",
        "        for table_tag_1 in table_tag.find_all('tbody', style= None):\n",
        "            for table_tag_2 in table_tag_1.find_all('tr', style= None):\n",
        "                for table_tag_3 in table_tag_2.find_all('td', style= None):\n",
        "                    for table_tag_4 in table_tag_3.find_all('p', style= None):\n",
        "                        for table_tag_5 in table_tag_4.find_all('span', style=\"font-size:14px\"):\n",
        "                            for table_tag_6 in table_tag_5.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\"):\n",
        "                                for table_tag_7 in table_tag_6.find_all('strong', ):\n",
        "                                        # print(table_tag_7.text.lower())\n",
        "                                        table_tag_7.decompose()\n",
        "\n",
        "    for table_tag in soup.find_all('p', style= None):\n",
        "        if table_tag.find('img'):\n",
        "            for table_tag_1 in table_tag.find_all('strong', style= None):\n",
        "                for table_tag_2 in table_tag_1.find_all('span', style=\"font-size:14px\"):\n",
        "                    for table_tag_3 in table_tag_2.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\"):\n",
        "                        for table_tag_4 in table_tag_3.find_all('a', ):\n",
        "                            # print(table_tag_4.text.lower())\n",
        "                            table_tag_4.decompose()\n",
        "\n",
        "    # xóa dạng toàn văn file word luật doanh nghiệp và văn bản hướng dẫn năm 2024\n",
        "    # Tìm tất cả các thẻ <p> với style=\"text-align:justify\"\n",
        "    for table_tag in soup.find_all('table', style= None):\n",
        "        for table_tag_1 in table_tag.find_all('tbody', style= None):\n",
        "            for table_tag_2 in table_tag_1.find_all('tr', style= None):\n",
        "                if table_tag_2.find('img'):\n",
        "                    for table_tag_3 in table_tag_2.find_all('td', style= None):\n",
        "                        # for table_tag_4 in table_tag_3.find_all('p', style= None):\n",
        "                        for table_tag_4 in table_tag_3.find_all('span', style=\"font-size:14px\"):\n",
        "                            for table_tag_5 in table_tag_4.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\"):\n",
        "                                for table_tag_6 in table_tag_5.find_all('strong', ):\n",
        "                                        # print(table_tag_6.text.lower())\n",
        "                                        table_tag_6.decompose()\n",
        "\n",
        "    for table_tag in soup.find_all('p', style=\"margin-right:8px; text-align:center\"):\n",
        "        for table_tag_1 in table_tag.find_all('span', style=\"font-size:14px\"):\n",
        "            for table_tag_2 in table_tag_1.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\"):\n",
        "                for table_tag_3 in table_tag_2.find_all('em'):\n",
        "                        # print(table_tag_3.text.lower())\n",
        "                        table_tag_3.decompose()\n",
        "\n",
        "    for table_tag in soup.find_all('p', style=\"text-align:center\", recursive=False):\n",
        "        for table_tag_1 in table_tag.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\", recursive=False):\n",
        "                for table_tag_2 in table_tag_1.find_all('span', style=\"font-size:14px\", recursive=False):\n",
        "                    if not table_tag_2.find_all(True):\n",
        "                        # print(table_tag_2.text.lower())\n",
        "                        table_tag_2.decompose()\n",
        "\n",
        "    # nguồn internet\n",
        "    for table_tag in soup.find_all('p', style=\"text-align:center\", recursive=False):\n",
        "        for table_tag_1 in table_tag.find_all('span', style=\"font-size:14px\", recursive=False):\n",
        "            for table_tag_2 in table_tag_1.find_all('span', style=\"font-family:Arial,Helvetica,sans-serif\", recursive=False):\n",
        "                table_tag_3 = table_tag_2.find('strong')\n",
        "                if not table_tag_3:\n",
        "                    # print(table_tag_2.text.lower())\n",
        "                    table_tag_2.decompose()\n",
        "\n",
        "    # file word các luật nổi bật và văn bản hướng dẫn thi hành (còn hiệu lực)\n",
        "    for table_tag in soup.find_all('strong'):\n",
        "        for table_tag_1 in table_tag.find_all('span', style=\"color:#00b1e1\"):\n",
        "                # print(table_tag_1.text.lower())\n",
        "                table_tag_1.decompose()\n",
        "    # nguồn từ internet)\n",
        "    for p_tag in soup.find_all('p', style=\"text-align:center\"):\n",
        "        for em_tag in p_tag.find_all('em'):\n",
        "            # print(em_tag.text.lower())\n",
        "            em_tag.decompose()\n",
        "\n",
        "    for table_tag in soup.find_all('p', style=\"margin-right:-2px; text-align:center\"):\n",
        "        for em_tag in table_tag.find_all('em'):\n",
        "            # print(em_tag.text.lower())\n",
        "            em_tag.decompose()\n",
        "\n",
        "    questions_and_answers1 = []  # Danh sách để lưu trữ các link và anchor cho từng đoạn h2\n",
        "    # Tìm tất cả các thẻ <h2>\n",
        "    h2_tags = soup.find_all('h3')\n",
        "    # h2_tags_real = soup.find_all('h2')\n",
        "    # Lặp qua từng cặp thẻ <h2>\n",
        "    for i in range(len(h2_tags)):\n",
        "        h2_current = h2_tags[i]\n",
        "        if i + 1 < len(h2_tags):\n",
        "            h2_next = h2_tags[i + 1]\n",
        "        else:\n",
        "            h2_next = 'Xuan Vinh Gay'\n",
        "        patterns = r\"^\\d+\\.\\d+\\.\\s*\"\n",
        "        # Tìm và thay thế phần (1) và phần 1. bằng chuỗi rỗng\n",
        "        second_part = re.sub(patterns, \"\", h2_current.text).strip()\n",
        "        answer_text = \"\"\n",
        "        # next_tag1 = h2_current.find_next_sibling()\n",
        "        # while next_tag1 and next_tag1.name != 'h2' and next_tag1.name != 'h3':\n",
        "        #     answer = unicodedata.normalize(\"NFC\", next_tag1.text)\n",
        "        #     answer_text += answer + \"\\n\"\n",
        "        #     next_tag1 = next_tag1.find_next_sibling()\n",
        "            \n",
        "        next_element = h2_current.next_sibling\n",
        "        while next_element and (not isinstance(next_element, Tag) or (next_element.name != 'h2' and next_element.name != 'h3')):\n",
        "            if next_element and next_element.name == 'table':\n",
        "                answer_text += table_to_markdown(next_element) + \"\\n\"\n",
        "                # answer_text += answer.strip() + \"\\n\"\n",
        "                # Kiểm tra nếu next_element là NavigableString\n",
        "                # print(answer_text)\n",
        "            elif isinstance(next_element, NavigableString):\n",
        "                answer = unicodedata.normalize(\"NFC\", next_element)\n",
        "                answer_text += answer.strip() + \"\\n\"\n",
        "            elif next_element and next_element.name:\n",
        "                answer = unicodedata.normalize(\"NFC\", next_element.get_text())\n",
        "                answer_text += answer.strip() + \"\\n\"\n",
        "            next_element = next_element.next_sibling\n",
        "        # Tìm tất cả các thẻ giữa h2_current và h2_next\n",
        "        siblings = []\n",
        "        next_tag = h2_current.find_next_sibling()\n",
        "        # while next_tag and next_tag != h2_next:\n",
        "        while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3':\n",
        "            siblings.append(next_tag)\n",
        "            next_tag = next_tag.find_next_sibling()\n",
        "\n",
        "        # Lặp qua các thẻ từ dưới lên trên trong các thẻ anh chị em và lưu kết quả tạm thời\n",
        "        temp_results = []  # Danh sách tạm thời để lưu trữ các kết quả\n",
        "        skip_next_p_check = True  # Biến cờ để kiểm tra thẻ <p> tiếp theo\n",
        "        for tag in reversed(siblings):\n",
        "            if tag.name == 'p':  # Kiểm tra nếu tag là thẻ <p>\n",
        "                p_tag = tag.text\n",
        "                # Kiểm tra xem nội dung của tag có bắt đầu bằng '(' và kết thúc bằng ')'\n",
        "                if tag and tag.find('a') and p_tag.strip().startswith('Căn cứ pháp lý:'):\n",
        "                    if skip_next_p_check:\n",
        "                        for a_tag in tag.find_all('a'):\n",
        "                            href = a_tag.get('href')\n",
        "                            if href and '.aspx' in href:\n",
        "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                test = p_tag.split(a_tag.get_text())\n",
        "                                if \"điều\" in test[0].lower():\n",
        "                                    p_tag = p_tag.replace(test[0], '', 1)\n",
        "                                    final = processing_ccpl_full(test[0])\n",
        "                                    # final = process_dieu_parts(test[0])\n",
        "                                    # modified_text = replace_dieu_in_text(test[0], final)\n",
        "                                    positions = split_by_dieu(final)\n",
        "                                    data_ccpl = []\n",
        "                                    for dieu in positions:\n",
        "                                        if \"điều\" in dieu.lower():\n",
        "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
        "                                            # Tìm anchor trong href\n",
        "                                            khoan_list = split_by_khoan(dieu)\n",
        "                                            for khoan in khoan_list:\n",
        "                                                if \"khoản\" in khoan.lower():\n",
        "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
        "                                                    if \"điểm\" in khoan.lower():\n",
        "                                                        diem_dict = extract_diem(khoan)\n",
        "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
        "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
        "                                            data_ccpl.append(dieu_dict)\n",
        "                                            # print(data_ccpl)\n",
        "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
        "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
        "                                            if len(ccpl_temp) > 3:\n",
        "                                                for i in range(0, len(ccpl_temp), 3):\n",
        "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
        "                                                    chunk = ccpl_temp[i:i+3]\n",
        "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                    if len(chunk) > 0:\n",
        "                                                        dictionary = {\n",
        "                                                            \"url\": href,\n",
        "                                                            \"anchor\": anchor,\n",
        "                                                            \"LawID\": law_id,\n",
        "                                                            \"LawTitle\": a_tag.get_text(),\n",
        "                                                            \"Dieu\": chunk[0],\n",
        "                                                            \"Khoan\": chunk[1],\n",
        "                                                            \"Diem\": chunk[2]\n",
        "                                                        }\n",
        "                                                        temp_results.append(dictionary)\n",
        "                                            else:\n",
        "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                if len(ccpl_temp) > 0:\n",
        "                                                    dictionary = {\n",
        "                                                        \"url\": href,\n",
        "                                                        \"anchor\": anchor,\n",
        "                                                        \"LawID\": law_id,\n",
        "                                                        \"LawTitle\": a_tag.get_text(),\n",
        "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
        "                                                        \"Khoan\": ccpl_temp[1],\n",
        "                                                        \"Diem\": ccpl_temp[2]\n",
        "                                                    }\n",
        "                                                    temp_results.append(dictionary)\n",
        "\n",
        "                                else:\n",
        "                                    temp_else = {}\n",
        "                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                    law_title = a_tag.get_text()\n",
        "                                    temp_else[\"url\"] = href\n",
        "                                    temp_else[\"anchor\"] = anchor\n",
        "                                    temp_else[\"LawID\"] = law_id\n",
        "                                    temp_else[\"LawTitle\"] = law_title\n",
        "                                    temp_else[\"Dieu\"] = \"0\"\n",
        "                                    temp_else[\"Khoan\"] = \"0\"\n",
        "                                    temp_else[\"Diem\"] = \"0\"\n",
        "                                    temp_results.append(temp_else)\n",
        "                    skip_next_p_check = False\n",
        "        ccpls = []\n",
        "        ccpls.extend(temp_results)\n",
        "\n",
        "        questions_and_answers1.append({\n",
        "            \"question\": re.sub(r'\\d+\\.\\s', '', h2_current.find_previous('h2').text) + ' ' + second_part.lower(),\n",
        "            \"answer\": re.sub(r'^(tải|xem).*tại đây\\.?$', '', answer_text.strip(), flags=re.IGNORECASE | re.MULTILINE),\n",
        "            \"links\": [],\n",
        "            \"ccpls\": ccpls\n",
        "        })\n",
        "    # Danh sách các mẫu regex cần tìm và xóa từ đó đến hết câu trả lời\n",
        "    patterns = [\n",
        "        re.compile(r\"\\s*\\n{2}\\s*Về\\s*vấn\\s*đề\\s*này,\"),  # Khớp với ít nhất 2 ký tự xuống dòng trước và giữa các từ\n",
        "        re.compile(r\"Về vấn đề này,\\s*THƯ VIỆN PHÁP LUẬT\\s*giải đáp như sau:\"),\n",
        "        re.compile(r\"\\*\\*\\*\\*\", re.MULTILINE),\n",
        "        re.compile(r\"THƯ VIỆN PHÁP LUẬT\"),\n",
        "        re.compile(r\"^Xem thêm.*\", re.MULTILINE),\n",
        "        re.compile(r\"^>> Xem.*\", re.MULTILINE),\n",
        "        re.compile(r\"^>>Xem.*\", re.MULTILINE),\n",
        "        re.compile(r\"^>> Quý khách.*\", re.MULTILINE),\n",
        "        re.compile(r\"^Xem chi tiết tại.*\", re.MULTILINE),\n",
        "        re.compile(r\"^>> Tham khảo.*\", re.MULTILINE),\n",
        "        re.compile(r\"^Tham khảo.*\", re.MULTILINE),\n",
        "        re.compile(r\"^Chọn lĩnh vực để xem văn bản liên quan.*\", re.MULTILINE),\n",
        "        re.compile(r\"^Trân trọng.*\", re.MULTILINE)\n",
        "    ]\n",
        "\n",
        "    # Vòng lặp qua từng cặp câu hỏi và câu trả lời\n",
        "    for qa_pair in questions_and_answers1:\n",
        "        # Lấy câu trả lời từ cặp câu hỏi và câu trả lời\n",
        "        answer = qa_pair[\"answer\"]\n",
        "        # Thay thế từ \"tại đây\" thành rỗng trong câu trả lời\n",
        "        cleaned_answer = re.sub(r'.*>\\s*tại đây\\s*<.*', '', answer, flags=re.IGNORECASE | re.MULTILINE)\n",
        "        # Cập nhật câu trả lời đã được thay thế trong cặp câu hỏi và câu trả lời\n",
        "        qa_pair[\"answer\"] = cleaned_answer\n",
        "\n",
        "        # Khởi tạo start_index với giá trị -1 (không tìm thấy)\n",
        "        start_index = -1\n",
        "        # Kiểm tra từng mẫu regex và cập nhật start_index nếu tìm thấy\n",
        "        for pattern in patterns:\n",
        "            match = pattern.search(cleaned_answer)\n",
        "            if match:\n",
        "                index = match.start()\n",
        "                # Nếu start_index chưa được cập nhật hoặc tìm thấy index nhỏ hơn (gần đầu chuỗi hơn)\n",
        "                if start_index == -1 or index < start_index:\n",
        "                    start_index = index\n",
        "\n",
        "        # Nếu tìm thấy bất kỳ mẫu nào, xóa từ đoạn đó đến hết câu trả lời\n",
        "        if start_index != -1:\n",
        "            cleaned_answer = cleaned_answer[:start_index].strip()\n",
        "            qa_pair[\"answer\"] = cleaned_answer\n",
        "\n",
        "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "    return json.dumps(questions_and_answers1, ensure_ascii=False, indent=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cj86rRJ_NX7i"
      },
      "outputs": [],
      "source": [
        "def processing_2(data):\n",
        "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "\n",
        "    # Tìm tất cả các thẻ <h2>\n",
        "    h2_tags = soup.find_all('h3')\n",
        "    h2_tags_real = soup.find_all('h2')\n",
        "    questions_and_answers2 = []\n",
        "    # Lặp qua từng cặp thẻ <h2>\n",
        "    for i in range(len(h2_tags)):\n",
        "        h2_current = h2_tags[i]\n",
        "        if i + 1 < len(h2_tags):\n",
        "            h2_next = h2_tags[i + 1]\n",
        "        else:\n",
        "            h2_next = 'Xuan Vinh Gay'\n",
        "        # Tìm tất cả các thẻ giữa h2_current và h2_next\n",
        "        siblings = []\n",
        "        next_tag = h2_current.find_next_sibling()\n",
        "        while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3':\n",
        "            siblings.append(next_tag)\n",
        "            next_tag = next_tag.find_next_sibling()\n",
        "\n",
        "        # Lặp qua các thẻ từ dưới lên trên trong các thẻ anh chị em và lưu kết quả tạm thời\n",
        "        temp_results = []  # Danh sách tạm thời để lưu trữ các kết quả\n",
        "        skip_next_p_check = True  # Biến cờ để kiểm tra thẻ <p> tiếp theo\n",
        "        for tag in reversed(siblings):\n",
        "            if tag.name == 'p':  # Kiểm tra nếu tag là thẻ <p>\n",
        "                p_tag = tag.text\n",
        "                # Kiểm tra xem nội dung của tag có bắt đầu bằng '(' và kết thúc bằng ')'\n",
        "                if tag and tag.find('a') and not p_tag.strip().startswith('>>') and p_tag.strip().startswith('(') and (p_tag.strip().endswith(')') or p_tag.strip().endswith(').')) and not (re.search(r'^\\(\\d+\\)|^\\((?i:[ivxlcdm]+)\\)', p_tag)):\n",
        "                    if skip_next_p_check:\n",
        "                        for a_tag in tag.find_all('a'):\n",
        "                            href = a_tag.get('href')\n",
        "                            if href and '.aspx' in href:\n",
        "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                test = p_tag.split(a_tag.get_text())\n",
        "                                if \"điều\" in test[0].lower():\n",
        "                                    p_tag = p_tag.replace(test[0], '', 1)\n",
        "                                    final = processing_ccpl_full(test[0])\n",
        "                                    # final = process_dieu_parts(test[0])\n",
        "                                    # modified_text = replace_dieu_in_text(test[0], final)\n",
        "                                    positions = split_by_dieu(final)\n",
        "                                    data_ccpl = []\n",
        "                                    for dieu in positions:\n",
        "                                        if \"điều\" in dieu.lower():\n",
        "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
        "                                            # Tìm anchor trong href\n",
        "                                            khoan_list = split_by_khoan(dieu)\n",
        "                                            for khoan in khoan_list:\n",
        "                                                if \"khoản\" in khoan.lower():\n",
        "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
        "                                                    if \"điểm\" in khoan.lower():\n",
        "                                                        diem_dict = extract_diem(khoan)\n",
        "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
        "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
        "                                            data_ccpl.append(dieu_dict)\n",
        "                                            # print(data_ccpl)\n",
        "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
        "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
        "                                            if len(ccpl_temp) > 3:\n",
        "                                                for i in range(0, len(ccpl_temp), 3):\n",
        "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
        "                                                    chunk = ccpl_temp[i:i+3]\n",
        "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                    if len(chunk) > 0:\n",
        "                                                        dictionary = {\n",
        "                                                            \"url\": href,\n",
        "                                                            \"anchor\": anchor,\n",
        "                                                            \"LawID\": law_id,\n",
        "                                                            \"LawTitle\": a_tag.get_text(),\n",
        "                                                            \"Dieu\": chunk[0],\n",
        "                                                            \"Khoan\": chunk[1],\n",
        "                                                            \"Diem\": chunk[2]\n",
        "                                                        }\n",
        "                                                        temp_results.append(dictionary)\n",
        "                                            else:\n",
        "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                if len(ccpl_temp) > 0:\n",
        "                                                    dictionary = {\n",
        "                                                        \"url\": href,\n",
        "                                                        \"anchor\": anchor,\n",
        "                                                        \"LawID\": law_id,\n",
        "                                                        \"LawTitle\": a_tag.get_text(),\n",
        "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
        "                                                        \"Khoan\": ccpl_temp[1],\n",
        "                                                        \"Diem\": ccpl_temp[2]\n",
        "                                                    }\n",
        "                                                    temp_results.append(dictionary)\n",
        "\n",
        "                                else:\n",
        "                                    temp_else = {}\n",
        "                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                    law_title = a_tag.get_text()\n",
        "                                    temp_else[\"url\"] = href\n",
        "                                    temp_else[\"anchor\"] = anchor\n",
        "                                    temp_else[\"LawID\"] = law_id\n",
        "                                    temp_else[\"LawTitle\"] = law_title\n",
        "                                    temp_else[\"Dieu\"] = \"0\"\n",
        "                                    temp_else[\"Khoan\"] = \"0\"\n",
        "                                    temp_else[\"Diem\"] = \"0\"\n",
        "                                    temp_results.append(temp_else)\n",
        "                    skip_next_p_check = False\n",
        "        ccpls = []\n",
        "        ccpls.extend(temp_results)\n",
        "\n",
        "        questions_and_answers2.append({\n",
        "            \"ccpls\": ccpls\n",
        "        })\n",
        "\n",
        "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "    # print(json.dumps(cleaned_questions_and_answers, ensure_ascii=False, indent=5))\n",
        "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "    return json.dumps(questions_and_answers2, ensure_ascii=False, indent=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d8I_taagNX7j"
      },
      "outputs": [],
      "source": [
        "def processing_3(data):\n",
        "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "\n",
        "    # # Tìm tất cả các thẻ <h2>\n",
        "    h2_tags = soup.find_all('h3')\n",
        "    h2_tags_real = soup.find_all('h2')\n",
        "    questions_and_answers3 = []\n",
        "    for i in range(len(h2_tags)):\n",
        "        h2_current = h2_tags[i]\n",
        "        # if i + 1 < len(h2_tags):\n",
        "        #     h2_next = h2_tags[i + 1]\n",
        "        # else:\n",
        "        #     h2_next = 'Xuan Vinh Gay'\n",
        "        temp_results = []\n",
        "        next_tag = h2_current.find_next_sibling()\n",
        "        found_first_p = False\n",
        "        while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3':\n",
        "            if next_tag.name == 'p' and not found_first_p:\n",
        "                p_tag = next_tag.text\n",
        "                if next_tag.find('a') and not next_tag.text.startswith('Xem thêm') and not next_tag.text.startswith('>>') and not next_tag.text.startswith('(') and not (re.search(r'^\\(\\d+\\)|^\\((?i:[ivxlcdm]+)\\)', p_tag)) and next_tag.text.endswith(':'):\n",
        "                    for a_tag in next_tag.find_all('a'):\n",
        "                        href = a_tag.get('href')\n",
        "                        if href and '.aspx' in href:\n",
        "                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                            test = p_tag.split(a_tag.get_text())\n",
        "                            if \"điều\" in test[0].lower():\n",
        "                                p_tag = p_tag.replace(test[0], '', 1)\n",
        "                                final = processing_ccpl_full(test[0])\n",
        "                                # final = process_dieu_parts(test[0])\n",
        "                                # modified_text = replace_dieu_in_text(test[0], final)\n",
        "                                positions = split_by_dieu(final)\n",
        "                                data_ccpl = []\n",
        "                                for dieu in positions:\n",
        "                                    if \"điều\" in dieu.lower():\n",
        "                                        dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
        "                                        # Tìm anchor trong href\n",
        "                                        khoan_list = split_by_khoan(dieu)\n",
        "                                        for khoan in khoan_list:\n",
        "                                            if \"khoản\" in khoan.lower():\n",
        "                                                khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
        "                                                if \"điểm\" in khoan.lower():\n",
        "                                                    diem_dict = extract_diem(khoan)\n",
        "                                                    khoan_dict[\"Diem\"].append(diem_dict)\n",
        "                                                dieu_dict[\"Khoan\"].append(khoan_dict)\n",
        "                                        data_ccpl.append(dieu_dict)\n",
        "                                        # print(data_ccpl)\n",
        "                                        # Tạo một từ điển mới và gán giá trị từ mảng\n",
        "                                        ccpl_temp = processing_output_ccpl(data_ccpl)\n",
        "                                        if len(ccpl_temp) > 3:\n",
        "                                            for i in range(0, len(ccpl_temp), 3):\n",
        "                                                # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
        "                                                chunk = ccpl_temp[i:i+3]\n",
        "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                if len(chunk) > 0:\n",
        "                                                    dictionary = {\n",
        "                                                        \"url\": href,\n",
        "                                                        \"anchor\": anchor,\n",
        "                                                        \"LawID\": law_id,\n",
        "                                                        \"LawTitle\": a_tag.get_text(),\n",
        "                                                        \"Dieu\": chunk[0],\n",
        "                                                        \"Khoan\": chunk[1],\n",
        "                                                        \"Diem\": chunk[2]\n",
        "                                                    }\n",
        "                                                    temp_results.append(dictionary)\n",
        "                                        else:\n",
        "                                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                            law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                            law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                            if len(ccpl_temp) > 0:\n",
        "                                                dictionary = {\n",
        "                                                    \"url\": href,\n",
        "                                                    \"anchor\": anchor,\n",
        "                                                    \"LawID\": law_id,\n",
        "                                                    \"LawTitle\": a_tag.get_text(),\n",
        "                                                    \"Dieu\": ccpl_temp[0] or \"0\",\n",
        "                                                    \"Khoan\": ccpl_temp[1],\n",
        "                                                    \"Diem\": ccpl_temp[2]\n",
        "                                                }\n",
        "                                                temp_results.append(dictionary)\n",
        "\n",
        "                            else:\n",
        "                                temp_else = {}\n",
        "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                law_title = a_tag.get_text()\n",
        "                                temp_else[\"url\"] = href\n",
        "                                temp_else[\"anchor\"] = anchor\n",
        "                                temp_else[\"LawID\"] = law_id\n",
        "                                temp_else[\"LawTitle\"] = law_title\n",
        "                                temp_else[\"Dieu\"] = \"0\"\n",
        "                                temp_else[\"Khoan\"] = \"0\"\n",
        "                                temp_else[\"Diem\"] = \"0\"\n",
        "                                temp_results.append(temp_else)\n",
        "                found_first_p = True  # Đã tìm thấy thẻ p đầu tiên\n",
        "            next_tag = next_tag.find_next_sibling()\n",
        "        ccpls = []\n",
        "        ccpls.extend(temp_results)\n",
        "        questions_and_answers3.append({\n",
        "            # \"question\": second_part,\n",
        "            # \"answer\": re.sub(r'^(tải|xem).*tại đây\\.?$', '', answer_text.strip(), flags=re.IGNORECASE | re.MULTILINE),\n",
        "            \"ccpls\": ccpls\n",
        "        })\n",
        "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "    return json.dumps(questions_and_answers3, ensure_ascii=False, indent=5)\n",
        "    # print(json.dumps(questions_and_answers3, ensure_ascii=False, indent=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LON2VMBfNX7k"
      },
      "outputs": [],
      "source": [
        "# def processing_4(data):\n",
        "#     soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "\n",
        "#     # # Tìm tất cả các thẻ <h2>\n",
        "#     h2_tags = soup.find_all('h3')\n",
        "#     h2_tags_real = soup.find_all('h2')\n",
        "#     questions_and_answers4 = []\n",
        "#     # Lặp qua từng cặp thẻ <h2>\n",
        "#     for i in range(len(h2_tags)):\n",
        "#         h2_current = h2_tags[i]\n",
        "#         if i + 1 < len(h2_tags):\n",
        "#             h2_next = h2_tags[i + 1]\n",
        "#         else:\n",
        "#             h2_next = 'Xuan Vinh Gay'\n",
        "#         # # Tách phần văn bản của thẻ <h2>\n",
        "#         # h2_text_parts = h2_current.text.split('.', 1)\n",
        "#         # if len(h2_text_parts) > 1 and h2_text_parts[1] != '':\n",
        "#         #     second_part = h2_text_parts[1].strip()\n",
        "#         # else:\n",
        "#         #     second_part = h2_current.text\n",
        "\n",
        "#         # Tìm tất cả các thẻ giữa h2_current và h2_next\n",
        "#         siblings = []\n",
        "#         next_tag = h2_current.find_next_sibling()\n",
        "#         # while next_tag and next_tag != h2_next:\n",
        "#         while next_tag and next_tag != h2_next and next_tag.name != 'h3': \n",
        "#             siblings.append(next_tag)\n",
        "#             next_tag = next_tag.find_next_sibling()\n",
        "\n",
        "#         # Lặp qua các thẻ từ dưới lên trên trong các thẻ anh chị em và lưu kết quả tạm thời\n",
        "#         temp_results = []  # Danh sách tạm thời để lưu trữ các kết quả\n",
        "#         reversed_temp_results = [] #mới thêm\n",
        "#         skip_next_p_check = True  # Biến cờ để kiểm tra thẻ <p> tiếp theo\n",
        "#         for tag in reversed(siblings):\n",
        "#             if tag.name == 'p':  # Kiểm tra nếu tag là thẻ <p>\n",
        "#                 count_a = 0\n",
        "#                 p_tag = tag.text\n",
        "#                 # Kiểm tra xem nội dung của tag có bắt đầu bằng '(' và kết thúc bằng ')'\n",
        "#                 if tag and tag.find('a') and p_tag.strip().startswith('Xem thêm') and not p_tag.strip().endswith(':'):\n",
        "#                     if skip_next_p_check:\n",
        "#                         for a_tag in tag.find_all('a'):\n",
        "#                             href = a_tag.get('href')\n",
        "#                             if href and '.aspx' in href:\n",
        "#                                 test = p_tag.split(a_tag.get_text())\n",
        "#                                 anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                                 anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                                 if \"điều\" in test[0].lower():\n",
        "#                                     p_tag = p_tag.replace(test[0], '', 1)\n",
        "#                                     final = processing_ccpl_full(test[0])\n",
        "#                                     # final = process_dieu_parts(test[0])\n",
        "#                                     # modified_text = replace_dieu_in_text(test[0], final)\n",
        "#                                     positions = split_by_dieu(final)\n",
        "#                                     data_ccpl = []\n",
        "#                                     for dieu in positions:\n",
        "#                                         if \"điều\" in dieu.lower():\n",
        "#                                             dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
        "#                                             # Tìm anchor trong href\n",
        "#                                             khoan_list = split_by_khoan(dieu)\n",
        "#                                             for khoan in khoan_list:\n",
        "#                                                 if \"khoản\" in khoan.lower():\n",
        "#                                                     khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
        "#                                                     if \"điểm\" in khoan.lower():\n",
        "#                                                         diem_dict = extract_diem(khoan)\n",
        "#                                                         khoan_dict[\"Diem\"].append(diem_dict)\n",
        "#                                                     dieu_dict[\"Khoan\"].append(khoan_dict)\n",
        "#                                             data_ccpl.append(dieu_dict)\n",
        "#                                             # print(data_ccpl)\n",
        "#                                             # Tạo một từ điển mới và gán giá trị từ mảng\n",
        "#                                             ccpl_temp = processing_output_ccpl(data_ccpl)\n",
        "#                                             if len(ccpl_temp) > 3:\n",
        "#                                                 for i in range(0, len(ccpl_temp), 3):\n",
        "#                                                     # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
        "#                                                     chunk = ccpl_temp[i:i+3]\n",
        "#                                                     anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                                                     law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "#                                                     anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                                                     law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "#                                                     if len(chunk) > 0:\n",
        "#                                                         dictionary = {\n",
        "#                                                             \"url\": href,\n",
        "#                                                             \"anchor\": anchor,\n",
        "#                                                             \"LawID\": law_id,\n",
        "#                                                             \"LawTitle\": a_tag.get_text(),\n",
        "#                                                             \"Dieu\": chunk[0],\n",
        "#                                                             \"Khoan\": chunk[1],\n",
        "#                                                             \"Diem\": chunk[2]\n",
        "#                                                         }\n",
        "#                                                         temp_results.append(dictionary)\n",
        "#                                             else:\n",
        "#                                                 anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                                                 law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "#                                                 anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                                                 law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "#                                                 if len(ccpl_temp) > 0:\n",
        "#                                                     dictionary = {\n",
        "#                                                         \"url\": href,\n",
        "#                                                         \"anchor\": anchor,\n",
        "#                                                         \"LawID\": law_id,\n",
        "#                                                         \"LawTitle\": a_tag.get_text(),\n",
        "#                                                         \"Dieu\": ccpl_temp[0] or \"0\",\n",
        "#                                                         \"Khoan\": ccpl_temp[1],\n",
        "#                                                         \"Diem\": ccpl_temp[2]\n",
        "#                                                     }\n",
        "#                                                     temp_results.append(dictionary)\n",
        "\n",
        "#                                 else:\n",
        "#                                     temp_else = {}\n",
        "#                                     anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                                     law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "#                                     anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                                     law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "#                                     law_title = a_tag.get_text()\n",
        "#                                     temp_else[\"url\"] = href\n",
        "#                                     temp_else[\"anchor\"] = anchor\n",
        "#                                     temp_else[\"LawID\"] = law_id\n",
        "#                                     temp_else[\"LawTitle\"] = law_title\n",
        "#                                     temp_else[\"Dieu\"] = \"0\"\n",
        "#                                     temp_else[\"Khoan\"] = \"0\"\n",
        "#                                     temp_else[\"Diem\"] = \"0\"\n",
        "#                                     temp_results.append(temp_else)\n",
        "#                                 reversed_temp_results = temp_results[::-1] # mới thêm\n",
        "#                     skip_next_p_check = False\n",
        "#         # Thêm kết quả từ danh sách tạm thời vào danh sách chính theo thứ tự ngược lại\n",
        "#         reversed_temp_results.reverse() #mới thêm\n",
        "#         # temp_results.reverse()  # Đảo ngược danh sách tạm thời để giữ nguyên thứ tự từ trên xuống dưới\n",
        "#         ccpls = []\n",
        "#         # ccpls.extend(temp_results)\n",
        "#         ccpls.extend(reversed_temp_results) # mới thêm\n",
        "\n",
        "#         questions_and_answers4.append({\n",
        "#             # \"question\": second_part,\n",
        "#             \"ccpls\": ccpls\n",
        "#         })\n",
        "#     # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "#     return json.dumps(questions_and_answers4, ensure_ascii=False, indent=5)\n",
        "#     # print(json.dumps(questions_and_answers3, ensure_ascii=False, indent=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ft50yl6JNX7l"
      },
      "outputs": [],
      "source": [
        "def processing_5(data):\n",
        "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "    h2_tags = soup.find_all('h3')\n",
        "    # h2_tags_real = soup.find_all('h2')\n",
        "    questions_and_answers5 = []  # Danh sách để lưu trữ các link và anchor cho từng đoạn h2\n",
        "\n",
        "\n",
        "    for i in range(len(h2_tags)):\n",
        "        h2_current = h2_tags[i]\n",
        "        # if i + 1 < len(h2_tags):\n",
        "        #     h2_next = h2_tags[i + 1]\n",
        "        # else:\n",
        "        #     h2_next = 'Xuan Vinh Gay'\n",
        "        temp_results = []\n",
        "        next_tag = h2_current.find_next_sibling()\n",
        "        gay_flag = True\n",
        "        # while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3':\n",
        "        while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3': \n",
        "            if next_tag.name == 'p':\n",
        "                p_tag = next_tag.text\n",
        "                if (re.search(r\"thư\\s+viện\\s+pháp\\s+luật\", next_tag.text.lower())):\n",
        "                    gay_flag = False\n",
        "                if next_tag.find('a') and not next_tag.text.startswith('>>') and gay_flag == True:\n",
        "                    for a_tag in next_tag.find_all('a'):\n",
        "                        href = a_tag.get('href')\n",
        "                        if href and '.aspx' in href:\n",
        "                            test = p_tag.split(a_tag.get_text())\n",
        "                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                            if \"điều\" in test[0].lower():\n",
        "                                #  test and test[0] is not None and\n",
        "                                p_tag = p_tag.replace(test[0], '', 1)\n",
        "                                final = processing_ccpl_full(test[0])\n",
        "                                # final = process_dieu_parts(test[0])\n",
        "                                # modified_text = replace_dieu_in_text(test[0], final)\n",
        "                                positions = split_by_dieu(final)\n",
        "                                data_ccpl = []\n",
        "                                for dieu in positions:\n",
        "                                        if \"điều\" in dieu.lower():\n",
        "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
        "                                            # Tìm anchor trong href\n",
        "                                            khoan_list = split_by_khoan(dieu)\n",
        "                                            for khoan in khoan_list:\n",
        "                                                if \"khoản\" in khoan.lower():\n",
        "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
        "                                                    if \"điểm\" in khoan.lower():\n",
        "                                                        diem_dict = extract_diem(khoan)\n",
        "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
        "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
        "                                            data_ccpl.append(dieu_dict)\n",
        "                                            # print(data_ccpl)\n",
        "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
        "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
        "                                            if len(ccpl_temp) > 3:\n",
        "                                                for i in range(0, len(ccpl_temp), 3):\n",
        "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
        "                                                    chunk = ccpl_temp[i:i+3]\n",
        "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                    # print(ccpl_temp)\n",
        "                                                    if len(chunk) > 0:\n",
        "                                                        dictionary = {\n",
        "                                                            \"url\": href,\n",
        "                                                            \"anchor\": anchor,\n",
        "                                                            \"LawID\": law_id,\n",
        "                                                            \"LawTitle\": a_tag.get_text(),\n",
        "                                                            \"Dieu\": chunk[0],\n",
        "                                                            \"Khoan\": chunk[1],\n",
        "                                                            \"Diem\": chunk[2]\n",
        "                                                        }\n",
        "                                                        temp_results.append(dictionary)\n",
        "                                            else:\n",
        "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                if len(ccpl_temp) > 0:\n",
        "                                                    dictionary = {\n",
        "                                                        \"url\": href,\n",
        "                                                        \"anchor\": anchor,\n",
        "                                                        \"LawID\": law_id,\n",
        "                                                        \"LawTitle\": a_tag.get_text(),\n",
        "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
        "                                                        \"Khoan\": ccpl_temp[1],\n",
        "                                                        \"Diem\": ccpl_temp[2]\n",
        "                                                    }\n",
        "                                                    temp_results.append(dictionary)\n",
        "\n",
        "                            else:\n",
        "                                temp_else = {}\n",
        "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                law_title = a_tag.get_text()\n",
        "                                temp_else[\"url\"] = href\n",
        "                                temp_else[\"anchor\"] = anchor\n",
        "                                temp_else[\"LawID\"] = law_id\n",
        "                                temp_else[\"LawTitle\"] = law_title\n",
        "                                temp_else[\"Dieu\"] = \"0\"\n",
        "                                temp_else[\"Khoan\"] = \"0\"\n",
        "                                temp_else[\"Diem\"] = \"0\"\n",
        "                                temp_results.append(temp_else)\n",
        "                    gay_flag = True\n",
        "            next_tag = next_tag.find_next_sibling()\n",
        "        ccpls = []\n",
        "        # ccpls.extend(temp_results)\n",
        "        ccpls.extend(temp_results)\n",
        "        questions_and_answers5.append({\n",
        "            \"ccpls\": ccpls\n",
        "        })\n",
        "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "    return json.dumps(questions_and_answers5, ensure_ascii=False, indent=5)\n",
        "# print(json.dumps(questions_and_answers4, ensure_ascii=False, indent=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DuZqKU4eNX7m"
      },
      "outputs": [],
      "source": [
        "def processing_6(data):\n",
        "    soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "    h2_tags = soup.find_all('h3')\n",
        "    # h2_tags_real = soup.find_all('h2')\n",
        "    questions_and_answers6 = []\n",
        "    for i in range(len(h2_tags)):\n",
        "        h2_current = h2_tags[i]\n",
        "        # if i + 1 < len(h2_tags):\n",
        "        #     h2_next = h2_tags[i + 1]\n",
        "        # else:\n",
        "        #     h2_next = 'Xuan Vinh Gay'\n",
        "        temp_results = []\n",
        "        next_tag = h2_current.find_next_sibling()\n",
        "        # while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3':\n",
        "        while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3': \n",
        "            if next_tag.name == 'p':\n",
        "                p_tag = next_tag.text\n",
        "                if next_tag.find('a') and (next_tag.text.lower().startswith('lưu ý') or next_tag.text.lower().startswith('đồng thời') or next_tag.text.lower().startswith('ngoài ra') or next_tag.text.lower().startswith('bên cạnh đó') or next_tag.text.lower().startswith('trừ trường hợp quy định tại') or next_tag.text.lower().startswith('tuy nhiên') or next_tag.text.lower().startswith('căn cứ') or next_tag.text.lower().startswith('(căn cứ')):\n",
        "                    for a_tag in next_tag.find_all('a'):\n",
        "                        href = a_tag.get('href')\n",
        "                        if href and '.aspx' in href:\n",
        "                            test = p_tag.split(a_tag.get_text())\n",
        "                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                            if \"điều\" in test[0].lower():\n",
        "                                p_tag = p_tag.replace(test[0], '', 1)\n",
        "                                final = processing_ccpl_full(test[0])\n",
        "                                # final = process_dieu_parts(test[0])\n",
        "                                # modified_text = replace_dieu_in_text(test[0], final)\n",
        "                                positions = split_by_dieu(final)\n",
        "                                data_ccpl = []\n",
        "                                for dieu in positions:\n",
        "                                        if \"điều\" in dieu.lower():\n",
        "                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
        "                                            # Tìm anchor trong href\n",
        "                                            khoan_list = split_by_khoan(dieu)\n",
        "                                            for khoan in khoan_list:\n",
        "                                                if \"khoản\" in khoan.lower():\n",
        "                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
        "                                                    if \"điểm\" in khoan.lower():\n",
        "                                                        diem_dict = extract_diem(khoan)\n",
        "                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
        "                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
        "                                            data_ccpl.append(dieu_dict)\n",
        "                                            # print(data_ccpl)\n",
        "                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
        "                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
        "                                            if len(ccpl_temp) > 3:\n",
        "                                                for i in range(0, len(ccpl_temp), 3):\n",
        "                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
        "                                                    chunk = ccpl_temp[i:i+3]\n",
        "                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                    if len(chunk) > 0:\n",
        "                                                        dictionary = {\n",
        "                                                            \"url\": href,\n",
        "                                                            \"anchor\": anchor,\n",
        "                                                            \"LawID\": law_id,\n",
        "                                                            \"LawTitle\": a_tag.get_text(),\n",
        "                                                            \"Dieu\": chunk[0],\n",
        "                                                            \"Khoan\": chunk[1],\n",
        "                                                            \"Diem\": chunk[2]\n",
        "                                                        }\n",
        "                                                        temp_results.append(dictionary)\n",
        "                                            else:\n",
        "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                                if len(ccpl_temp) > 0:\n",
        "                                                    dictionary = {\n",
        "                                                        \"url\": href,\n",
        "                                                        \"anchor\": anchor,\n",
        "                                                        \"LawID\": law_id,\n",
        "                                                        \"LawTitle\": a_tag.get_text(),\n",
        "                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
        "                                                        \"Khoan\": ccpl_temp[1],\n",
        "                                                        \"Diem\": ccpl_temp[2]\n",
        "                                                    }\n",
        "                                                    temp_results.append(dictionary)\n",
        "\n",
        "                            else:\n",
        "                                temp_else = {}\n",
        "                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "                                law_title = a_tag.get_text()\n",
        "                                temp_else[\"url\"] = href\n",
        "                                temp_else[\"anchor\"] = anchor\n",
        "                                temp_else[\"LawID\"] = law_id\n",
        "                                temp_else[\"LawTitle\"] = law_title\n",
        "                                temp_else[\"Dieu\"] = \"0\"\n",
        "                                temp_else[\"Khoan\"] = \"0\"\n",
        "                                temp_else[\"Diem\"] = \"0\"\n",
        "                                temp_results.append(temp_else)\n",
        "                # found_first_p = True  # Đã tìm thấy thẻ p đầu tiên\n",
        "            next_tag = next_tag.find_next_sibling()\n",
        "        ccpls = []\n",
        "        ccpls.extend(temp_results)\n",
        "        questions_and_answers6.append({\n",
        "            # \"question\": second_part,\n",
        "            # \"answer\": re.sub(r'^(tải|xem).*tại đây\\.?$', '', answer_text.strip(), flags=re.IGNORECASE | re.MULTILINE),\n",
        "            \"ccpls\": ccpls\n",
        "        })\n",
        "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "    return json.dumps(questions_and_answers6, ensure_ascii=False, indent=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def processing_7(data):\n",
        "#     soup = BeautifulSoup(data, 'html.parser', from_encoding= 'utf-8')\n",
        "#     h2_tags = soup.find_all('h3')\n",
        "#     h2_tags_real = soup.find_all('h2')\n",
        "#     questions_and_answers7 = []\n",
        "#     for i in range(len(h2_tags)):\n",
        "#         h2_current = h2_tags[i]\n",
        "#         if i + 1 < len(h2_tags):\n",
        "#             h2_next = h2_tags[i + 1]\n",
        "#         else:\n",
        "#             h2_next = 'Xuan Vinh Gay'\n",
        "#         temp_results = []\n",
        "#         next_tag = h2_current.find_previous_sibling('p')\n",
        "#         # print(h2_tag.text)\n",
        "#         # if next_tag:\n",
        "#         #     print(next_tag.text)\n",
        "#         skip_next_p_check = True\n",
        "#         # while next_tag and next_tag.name != 'h2' and next_tag.name != 'h3' and skip_next_p_check:\n",
        "#         while next_tag and next_tag != h2_next and next_tag.name != 'h3':\n",
        "#             if next_tag.name == 'p':\n",
        "#                 p_tag = next_tag.text\n",
        "#                 # print(p_tag)\n",
        "#                 if next_tag.find('a') and next_tag.text.lower().endswith(':'):\n",
        "#                     for a_tag in next_tag.find_all('a'):\n",
        "#                         href = a_tag.get('href')\n",
        "#                         if href and '.aspx' in href:\n",
        "#                             test = p_tag.split(a_tag.get_text())\n",
        "#                             anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                             anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                             if \"điều\" in test[0].lower():\n",
        "#                                 p_tag = p_tag.replace(test[0], '', 1)\n",
        "#                                 final = processing_ccpl_full(test[0])\n",
        "#                                 # final = process_dieu_parts(test[0])\n",
        "#                                 # modified_text = replace_dieu_in_text(test[0], final)\n",
        "#                                 positions = split_by_dieu(final)\n",
        "#                                 data_ccpl = []\n",
        "#                                 for dieu in positions:\n",
        "#                                         if \"điều\" in dieu.lower():\n",
        "#                                             dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
        "#                                             # Tìm anchor trong href\n",
        "#                                             khoan_list = split_by_khoan(dieu)\n",
        "#                                             for khoan in khoan_list:\n",
        "#                                                 if \"khoản\" in khoan.lower():\n",
        "#                                                     khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
        "#                                                     if \"điểm\" in khoan.lower():\n",
        "#                                                         diem_dict = extract_diem(khoan)\n",
        "#                                                         khoan_dict[\"Diem\"].append(diem_dict)\n",
        "#                                                     dieu_dict[\"Khoan\"].append(khoan_dict)\n",
        "#                                             data_ccpl.append(dieu_dict)\n",
        "#                                             # print(data_ccpl)\n",
        "#                                             # Tạo một từ điển mới và gán giá trị từ mảng\n",
        "#                                             ccpl_temp = processing_output_ccpl(data_ccpl)\n",
        "#                                             if len(ccpl_temp) > 3:\n",
        "#                                                 for i in range(0, len(ccpl_temp), 3):\n",
        "#                                                     # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
        "#                                                     chunk = ccpl_temp[i:i+3]\n",
        "#                                                     anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                                                     law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "#                                                     anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                                                     law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "#                                                     if len(chunk) > 0:\n",
        "#                                                         dictionary = {\n",
        "#                                                             \"url\": href,\n",
        "#                                                             \"anchor\": anchor,\n",
        "#                                                             \"LawID\": law_id,\n",
        "#                                                             \"LawTitle\": a_tag.get_text(),\n",
        "#                                                             \"Dieu\": chunk[0],\n",
        "#                                                             \"Khoan\": chunk[1],\n",
        "#                                                             \"Diem\": chunk[2]\n",
        "#                                                         }\n",
        "#                                                         temp_results.append(dictionary)\n",
        "#                                             else:\n",
        "#                                                 anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                                                 law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "#                                                 anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                                                 law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "#                                                 if len(ccpl_temp) > 0:\n",
        "#                                                     dictionary = {\n",
        "#                                                         \"url\": href,\n",
        "#                                                         \"anchor\": anchor,\n",
        "#                                                         \"LawID\": law_id,\n",
        "#                                                         \"LawTitle\": a_tag.get_text(),\n",
        "#                                                         \"Dieu\": ccpl_temp[0] or \"0\",\n",
        "#                                                         \"Khoan\": ccpl_temp[1],\n",
        "#                                                         \"Diem\": ccpl_temp[2]\n",
        "#                                                     }\n",
        "#                                                     temp_results.append(dictionary)\n",
        "\n",
        "#                             else:\n",
        "#                                 temp_else = {}\n",
        "#                                 anchor_match = re.search(r'anchor=(\\w+)', href)\n",
        "#                                 law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
        "#                                 anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
        "#                                 law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
        "#                                 law_title = a_tag.get_text()\n",
        "#                                 temp_else[\"url\"] = href\n",
        "#                                 temp_else[\"anchor\"] = anchor\n",
        "#                                 temp_else[\"LawID\"] = law_id\n",
        "#                                 temp_else[\"LawTitle\"] = law_title\n",
        "#                                 temp_else[\"Dieu\"] = \"0\"\n",
        "#                                 temp_else[\"Khoan\"] = \"0\"\n",
        "#                                 temp_else[\"Diem\"] = \"0\"\n",
        "#                                 temp_results.append(temp_else)\n",
        "#                 skip_next_p_check = False # Đã tìm thấy thẻ p đầu tiên\n",
        "#             next_tag = next_tag.find_previous_sibling('p')\n",
        "#         ccpls = []\n",
        "#         ccpls.extend(temp_results)\n",
        "#         questions_and_answers7.append({\n",
        "#             # \"question\": second_part,\n",
        "#             # \"answer\": re.sub(r'^(tải|xem).*tại đây\\.?$', '', answer_text.strip(), flags=re.IGNORECASE | re.MULTILINE),\n",
        "#             \"ccpls\": ccpls\n",
        "#         })\n",
        "#     # Lấy giá trị của phần tử ccpls đầu tiên\n",
        "#     non_empty_ccpls = []\n",
        "#     if questions_and_answers7 and questions_and_answers7[0][\"ccpls\"]:\n",
        "#         non_empty_ccpls = questions_and_answers7[0][\"ccpls\"]\n",
        "\n",
        "#     # Sao chép giá trị của phần tử ccpls đầu tiên vào các phần tử ccpls rỗng\n",
        "#     for item in questions_and_answers7:\n",
        "#         if not item[\"ccpls\"]:\n",
        "#             item[\"ccpls\"] = non_empty_ccpls\n",
        "\n",
        "#     # # Sao chép giá trị của ccpls không rỗng vào các phần tử ccpls rỗng\n",
        "#     # if non_empty_ccpls:\n",
        "#     #     for item in questions_and_answers7:\n",
        "#     #         if not item[\"ccpls\"]:\n",
        "#     #             item[\"ccpls\"] = non_empty_ccpls\n",
        "#     # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
        "#     return json.dumps(questions_and_answers7, ensure_ascii=False, indent=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4qt_goHhNX7m"
      },
      "outputs": [],
      "source": [
        "def extract_data(data):\n",
        "    links_data = json.loads(processing_link_table(data))\n",
        "    links1_data = json.loads(processing_link_p_tag(data))\n",
        "    file1_data = json.loads(processing_1(data))\n",
        "    file2_data = json.loads(processing_2(data))\n",
        "    file3_data = json.loads(processing_3(data))\n",
        "    # file4_data = json.loads(processing_4(data))\n",
        "    file5_data = json.loads(processing_5(data))\n",
        "    file6_data = json.loads(processing_6(data))\n",
        "    # file7_data = json.loads(processing_7(data))\n",
        "\n",
        "    # ghép link\n",
        "    for index, item in enumerate(file1_data):\n",
        "        item['links'] = links_data[index]['links']\n",
        "\n",
        "    # ghép link dạng 2\n",
        "    for index, item in enumerate(file1_data):\n",
        "        item['links'] = item['links'] + links1_data[index]['links']\n",
        "\n",
        "    # ghép ccpl loại 2\n",
        "    for index, item in enumerate(file1_data):\n",
        "        if not item['ccpls']:\n",
        "            item['ccpls'] = file2_data[index]['ccpls']\n",
        "\n",
        "    # ghép ccpl loại 3\n",
        "    for index, item in enumerate(file1_data):\n",
        "        if not item['ccpls']:\n",
        "            item['ccpls'] = file3_data[index]['ccpls']\n",
        "\n",
        "    # # ghép ccpl loại 4\n",
        "    # for index, item in enumerate(file1_data):\n",
        "    #     if not item['ccpls']:\n",
        "    #         item['ccpls'] = file4_data[index]['ccpls']\n",
        "\n",
        "    # ghép ccpl loại 5\n",
        "    for index, item in enumerate(file1_data):\n",
        "        if not item['ccpls']:\n",
        "            item['ccpls'] = file5_data[index]['ccpls']\n",
        "\n",
        "    # ghép ccpl loại 6\n",
        "    for index, item in enumerate(file1_data):\n",
        "        item['ccpls'] =  item['ccpls'] + file6_data[index]['ccpls']\n",
        "    \n",
        "    # # ghép ccpl loại 7\n",
        "    # for index, item in enumerate(file1_data):\n",
        "    #     item['ccpls'] =  item['ccpls'] + file7_data[index]['ccpls']\n",
        "\n",
        "    # Chỉ giữ lại các mục có câu trả lời\n",
        "    filtered_questions_and_answers = [qa for qa in file1_data if 'answer' in qa and qa['answer']]\n",
        "    filtered_questions_and_answers1 = [qa for qa in filtered_questions_and_answers if 'question' in qa and qa['question']]\n",
        "    questions_and_answers_final = []\n",
        "    for qa in filtered_questions_and_answers1:\n",
        "        question_text = qa[\"question\"]\n",
        "        # Kiểm tra xem câu hỏi có chứa các từ \"dự thảo\", \"đề xuất\", \"sắp tới\" hoặc \"dự kiến\" không\n",
        "        if not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', question_text.lower(), flags=re.IGNORECASE) and not question_text.lower().startswith(\"đã có\"):\n",
        "            questions_and_answers_final.append(qa)\n",
        "\n",
        "    # Remove duplicates\n",
        "    questions_and_answers_final = remove_duplicates_ccpls(questions_and_answers_final)\n",
        "    return questions_and_answers_final\n",
        "    # return json.dumps(questions_and_answers_final, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcVowYdtNX7n"
      },
      "source": [
        "function html request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FSWIj6OwNX7n"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import html\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def post_request(url, headers):\n",
        "    response = requests.post(url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "def post_request_key(url, headers, data):\n",
        "    try:\n",
        "        data = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "        data.raise_for_status()  # Raise an error for bad status codes\n",
        "        return data.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B4XLXNAFNX7n"
      },
      "outputs": [],
      "source": [
        "# Sử dụng hàm để thực hiện yêu cầu POST\n",
        "def get_api():\n",
        "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
        "    headers1 = {\n",
        "        'Cookie': 'Culture=vi; Culture=vi'\n",
        "    }\n",
        "\n",
        "    key = post_request(url1, headers1)\n",
        "\n",
        "    url = 'https://apids.thuvienphapluat.vn/data/get-phapluatdoanhnghiep-job'\n",
        "    headers = {\n",
        "        'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
        "        'Content-Type': 'application/json',\n",
        "        'Cookie': 'Culture=vi'\n",
        "    }\n",
        "    return url, headers\n",
        "\n",
        "def send_data_to_api(data):\n",
        "    url, headers = get_api()\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "POST data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# data = []\n",
        "# data_api = {\n",
        "#     \"page\": 1, #14039\n",
        "#     \"num\": 100\n",
        "# }\n",
        "# url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
        "# headers1 = {\n",
        "#     'Cookie': 'Culture=vi; Culture=vi'\n",
        "# }\n",
        "\n",
        "# key = post_request(url1, headers1)\n",
        "# url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
        "\n",
        "# headers_post = {\n",
        "# 'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
        "# 'Content-Type': 'application/json',\n",
        "# 'Cookie': 'Culture=vi'\n",
        "# }\n",
        "\n",
        "# url, headers = get_api()\n",
        "# response = post_request_key(url, headers, data_api)\n",
        "# # print(response.get('data'))\n",
        "# data_input = response.get('data')\n",
        "# for temp in data_input:\n",
        "#     # print(temp[\"content\"])\n",
        "#     extracted_data = extract_data(temp[\"content\"])\n",
        "#     # print(extract_data(temp[\"content\"]))\n",
        "#     if extracted_data != [] and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
        "#         dictionary = {\n",
        "#             \"objid\": int(temp['obj_id']),\n",
        "#             \"source\": str(temp['obj_code']),\n",
        "#             \"data\": extracted_data,\n",
        "#             \"type\": 1\n",
        "#         }\n",
        "#         dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
        "#         data.append(dictionary)\n",
        "#         # response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
        "#         print(dictionary)\n",
        "#         # time.sleep(1)\n",
        "# # now = specific_page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\bs4\\__init__.py:228: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
            "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n",
            "{\"messages\":\"success\"}\n"
          ]
        }
      ],
      "source": [
        "data_api = {\n",
        "    \"page\": 1,\n",
        "    \"num\": 1\n",
        "}\n",
        "data = []\n",
        "file_note = []\n",
        "url, headers = get_api()\n",
        "response = post_request_key(url, headers, data_api)\n",
        "now = (360)//100+1\n",
        "if response:\n",
        "    specific_page = response.get('total')\n",
        "\n",
        "num = 100\n",
        "total_page = specific_page // num + ( 1 if specific_page % num > 0 else 0)\n",
        "for num_page in range(now, total_page + 1):\n",
        "    data_api = {\n",
        "        \"page\": num_page, #14039\n",
        "        \"num\": num\n",
        "    }\n",
        "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
        "    headers1 = {\n",
        "        'Cookie': 'Culture=vi; Culture=vi'\n",
        "    }\n",
        "\n",
        "    key = post_request(url1, headers1)\n",
        "    url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
        "\n",
        "    headers_post = {\n",
        "    'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
        "    'Content-Type': 'application/json',\n",
        "    'Cookie': 'Culture=vi'\n",
        "    }\n",
        "\n",
        "    url, headers = get_api()\n",
        "    response = post_request_key(url, headers, data_api)\n",
        "    # print(response.get('data'))\n",
        "    data_input = response.get('data')\n",
        "    for temp in data_input:\n",
        "        # print(temp[\"content\"])\n",
        "        extracted_data = extract_data(temp[\"content\"])\n",
        "        # print(extract_data(temp[\"content\"]))\n",
        "        if extracted_data != [] and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
        "            dictionary = {\n",
        "                \"objid\": int(temp['obj_id']),\n",
        "                \"source\": str(temp['obj_code']+'_h3'),\n",
        "                \"data\": json.dumps(extracted_data, ensure_ascii=False),\n",
        "                \"type\": 1\n",
        "            }\n",
        "            dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
        "            data.append(dictionary)\n",
        "            response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
        "            print(response.text)\n",
        "        else:\n",
        "            file_note.append(int(temp['obj_id']))\n",
        "            # time.sleep(1)\n",
        "now = specific_page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data_PLDN_job_h3.txt', 'w') as file:\n",
        "    for item in data:\n",
        "        file.write(f\"{item}\\n\")\n",
        "        file.write(f\"\\n\")\n",
        "        file.write(f\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('file_note_h3_PLDN_job.txt', 'w') as file:\n",
        "    for item in file_note:\n",
        "        file.write(f\"{item}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "360"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "now"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "minh_python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
