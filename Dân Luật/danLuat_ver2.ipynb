{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup, NavigableString,Tag\n",
    "from tabulate import tabulate\n",
    "import unicodedata\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Điều 1, 2, 3 + replace\n",
    "import re\n",
    "\n",
    "def split_by_dieu_new1(text):\n",
    "    # def extract_dieu_positions(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "\n",
    "        \n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in dieu_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "################\n",
    "def extract_dieu_err(text):\n",
    "    # Define the pattern to match \"Điều\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "##############\n",
    "def process_dieu_parts(text):\n",
    "    dieu_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)Điều\\s+\\d+', temp):\n",
    "                    dieu_final.append(temp)\n",
    "                else:\n",
    "                    dieu_final.append('Điều ' + temp)\n",
    "\n",
    "    return dieu_final\n",
    "###############\n",
    "def replace_dieu_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_dieu_new1(text)\n",
    "    matches = extract_dieu_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    dieu_final = process_dieu_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)Điều\\s+\\d+', part) and \" và \" in part:\n",
    "            for dieu in dieu_final:\n",
    "                replaced_text.append(dieu)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "\n",
    "#############\n",
    "#############\n",
    "########### khoản 1, 2,3  và .... + replace\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def split_by_khoan_new1(text):\n",
    "    if text is None:\n",
    "        return[text]       \n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in khoan_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "\n",
    "#####################\n",
    "def extract_khoan_err(text):\n",
    "    # Define the pattern to match \"khoản\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "################\n",
    "def process_khoan_parts(text):\n",
    "    khoan_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    khoan_final.append(temp)\n",
    "                else:\n",
    "                    khoan_final.append('khoản ' + temp)\n",
    "\n",
    "    return khoan_final\n",
    "###############\n",
    "\n",
    "def replace_khoan_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_khoan_new1(text)\n",
    "    matches = extract_khoan_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    khoan_final = process_khoan_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for khoan in khoan_final:\n",
    "                replaced_text.append(khoan)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n",
    "#############\n",
    "#############\n",
    "################# Điểm 1,2,3 và .... + replace\n",
    "import re\n",
    "\n",
    "def split_by_diem_new1(text):\n",
    "    def extract_diem_positions(text):\n",
    "        return [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])', text, re.IGNORECASE)]\n",
    "        \n",
    "    diem_positions = extract_diem_positions(text)\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    for start, end in diem_positions:\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:start].strip())\n",
    "        split_text.append(text[start:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n",
    "###########################\n",
    "def extract_diem_err(text):\n",
    "    # Define the pattern to match \"điểm\" followed by numbers and possibly letters or other characters\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "\n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    matched_phrases = [match.group(0) for match in matches]\n",
    "    check_phrases = []\n",
    "\n",
    "    # Loop through each phrase in matched_phrases\n",
    "    for phrase in matched_phrases:\n",
    "        if ',' in phrase or ' và ' in phrase:\n",
    "            check_phrases.append(phrase)\n",
    "    return check_phrases\n",
    "#########################\n",
    "def process_diem_parts(text):\n",
    "    diem_final = []\n",
    "    for match in text:\n",
    "        if ',' in match or ' và ' in match:\n",
    "            splitted = re.split(r',| và ', match)\n",
    "            for temp in splitted:\n",
    "                if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', temp):\n",
    "                    diem_final.append(temp)\n",
    "                else:\n",
    "                    diem_final.append('điểm ' + temp)\n",
    "\n",
    "    return diem_final\n",
    "#########################\n",
    "def replace_diem_in_text(text):\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    split_text = split_by_diem_new1(text)\n",
    "    matches = extract_diem_err(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    diem_final = process_diem_parts(matches)\n",
    "    replaced_text = []\n",
    "    for part in split_text:\n",
    "        if re.match(r'(?i)điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)+)', part) and \" và \" in part:\n",
    "            for diem in diem_final:\n",
    "                replaced_text.append(diem)\n",
    "        else:\n",
    "            replaced_text.append(part)\n",
    "\n",
    "    return ' '.join(replaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_transform_dieu_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_dieu = None\n",
    "\n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ Điều\" in split_text_part:\n",
    "                start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if start_dieu_match:\n",
    "                    start_dieu = start_dieu_match.group(1)\n",
    "            elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "                end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "                if end_dieu_match:\n",
    "                    end_dieu = end_dieu_match.group(1)\n",
    "                    start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                    end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                    if start_index is not None and end_index is not None:\n",
    "                        for d in range(start_index, end_index + 1):\n",
    "                            new_split_text.append(f\"Điều {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ Điều\" in split_text_part:\n",
    "            start_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if start_dieu_match:\n",
    "                start_dieu = start_dieu_match.group(1)\n",
    "        elif \"đến Điều\" in split_text_part or \"tới Điều\" in split_text_part:\n",
    "            end_dieu_match = re.search(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE)\n",
    "            if end_dieu_match:\n",
    "                end_dieu = end_dieu_match.group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_dieu)) if start_dieu else None\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_dieu)) if end_dieu else None\n",
    "                if start_index is not None and end_index is not None:\n",
    "                    for d in range(start_index, end_index + 1):\n",
    "                        new_split_text.append(f\"Điều {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_dieu_ranges(text):\n",
    "    # Tìm khoảng \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    dieu_range_match = re.search(r'từ Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) Điều \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    if not dieu_range_match:\n",
    "        return text\n",
    "\n",
    "    dieu_range_text = dieu_range_match.group(0)\n",
    "    dieu_list = split_and_transform_dieu_ranges(dieu_range_text)\n",
    "    \n",
    "    # Tạo chuỗi mới từ danh sách các Điều\n",
    "    dieu_list_str = ', '.join(dieu_list)\n",
    "\n",
    "    # Thay thế đoạn văn bản \"từ Điều X đến Điều Y\" hoặc \"từ Điều X tới Điều Y\" bằng danh sách các Điều\n",
    "    new_text = text.replace(dieu_range_text, dieu_list_str)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_khoan_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "        \n",
    "\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_khoan = None\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ khoản\" in split_text_part.lower():\n",
    "                start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "                end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"khoản {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ khoản\" in split_text_part.lower():\n",
    "            start_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến khoản\" in split_text_part.lower() or \"tới khoản\" in split_text_part.lower():\n",
    "            end_khoan = re.search(r'khoản\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_khoan))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_khoan))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"khoản {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_khoan_ranges(text):\n",
    "    # Tìm khoảng \"từ khoản X đến khoản Y\" hoặc \"từ khoản X tới khoản Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    khoan_range_matches = re.finditer(r'từ khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) khoản \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    \n",
    "    new_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    for match in khoan_range_matches:\n",
    "        khoan_range_text = match.group(0)\n",
    "        khoan_list = split_and_transform_khoan_ranges(khoan_range_text)\n",
    "        \n",
    "        # Tạo chuỗi mới từ danh sách các khoản\n",
    "        khoan_list_str = ', '.join(khoan_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + khoan_list_str + new_text[end:]\n",
    "        \n",
    "        # Cập nhật offset\n",
    "        offset += len(khoan_list_str) - len(khoan_range_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def split_and_transform_diem_ranges(text):\n",
    "    if text is None:\n",
    "        return [text]\n",
    "        \n",
    "\n",
    "    diem_positions = [(match.start(), match.end()) for match in re.finditer(r'điểm\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not diem_positions:\n",
    "        return [text]\n",
    "\n",
    "    new_split_text = []\n",
    "    current_position = 0\n",
    "    start_diem = None\n",
    "\n",
    "    for i, (start, end) in enumerate(diem_positions):\n",
    "        if current_position < start:\n",
    "            split_text_part = text[current_position:end].strip()\n",
    "            if \"từ điểm\" in split_text_part.lower():\n",
    "                start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "                end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "                start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "                end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "                for d in range(start_index, end_index + 1):\n",
    "                    new_split_text.append(f\"điểm {d}\")\n",
    "            else:\n",
    "                new_split_text.append(split_text_part)\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text_part = text[current_position:].strip()\n",
    "        if \"từ điểm\" in split_text_part.lower():\n",
    "            start_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "        elif \"đến điểm\" in split_text_part.lower() or \"tới điểm\" in split_text_part.lower():\n",
    "            end_diem = re.search(r'điểm\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', split_text_part, re.IGNORECASE).group(1)\n",
    "            start_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", start_diem))\n",
    "            end_index = int(re.sub(\"[a-zA-ZđĐ]\", \"\", end_diem))\n",
    "            for d in range(start_index, end_index + 1):\n",
    "                new_split_text.append(f\"điểm {d}\")\n",
    "        else:\n",
    "            new_split_text.append(split_text_part)\n",
    "\n",
    "    return new_split_text\n",
    "\n",
    "def replace_diem_ranges(text):\n",
    "    # Tìm khoảng \"từ điểm X đến điểm Y\" hoặc \"từ điểm X tới điểm Y\"\n",
    "    if text is None:\n",
    "        return[text]\n",
    "    diem_range_matches = re.finditer(r'từ điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)* (?:đến|tới) điểm \\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)\n",
    "    \n",
    "    new_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    for match in diem_range_matches:\n",
    "        diem_range_text = match.group(0)\n",
    "        diem_list = split_and_transform_diem_ranges(diem_range_text)\n",
    "        \n",
    "        # Tạo chuỗi mới từ danh sách các điểm\n",
    "        diem_list_str = ', '.join(diem_list)\n",
    "\n",
    "        # Tính toán vị trí mới sau khi thay thế\n",
    "        start, end = match.start() + offset, match.end() + offset\n",
    "        new_text = new_text[:start] + diem_list_str + new_text[end:]\n",
    "        \n",
    "        # Cập nhật offset\n",
    "        offset += len(diem_list_str) - len(diem_range_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## dạng ..... 1, 2, 3 và .....,......\n",
    "def processing_ccpl_full(text):\n",
    "    if text is None:\n",
    "        return [text]  \n",
    "    check = replace_dieu_ranges(replace_khoan_ranges(replace_diem_ranges(text)))\n",
    "    check2 = replace_dieu_in_text(replace_khoan_in_text(replace_diem_in_text(check)))\n",
    "\n",
    "    return check2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ điều này tới điều kia\n",
    "\n",
    "\n",
    "def split_by_dieu(text):\n",
    "    if text is None: \n",
    "        return[text]\n",
    "    \n",
    "    text = ' ' + text\n",
    "    dieu_positions = [(match.start(), match.end()) for match in re.finditer(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', text, re.IGNORECASE)]\n",
    "    if not dieu_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "    \n",
    "    for i, (start, end) in enumerate(dieu_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Cắt từ khoản này tới khoản kia\n",
    "\n",
    "def split_by_khoan(text):\n",
    "    if text is None:\n",
    "            return[text]\n",
    "\n",
    "    text = ' ' + text\n",
    "    khoan_positions = [(match.start(), match.end()) for match in re.finditer(r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)', text, re.IGNORECASE)]\n",
    "    if not khoan_positions:\n",
    "        return [text]\n",
    "\n",
    "    split_text = []\n",
    "    current_position = 0\n",
    "\n",
    "    first_khoan_start = khoan_positions[0][0]\n",
    "    if first_khoan_start > 0:\n",
    "        split_text.append(text[:first_khoan_start].strip())\n",
    "\n",
    "    for i, (start, end) in enumerate(khoan_positions):\n",
    "        if current_position < start:\n",
    "            split_text.append(text[current_position:end].strip())\n",
    "        current_position = end\n",
    "\n",
    "    if current_position < len(text):\n",
    "        split_text.append(text[current_position:].strip())\n",
    "\n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm trích xuất điều\n",
    "def extract_dieu(text):\n",
    "    # Find individual \"Điều\" references\n",
    "    dieu_list = re.findall(r'Điều\\s+(\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)', text, re.IGNORECASE)\n",
    "\n",
    "    # Find joined \"Điều\" lists\n",
    "    joined_dieu_list = re.findall(r'Điều\\s+\\d+[a-zA-Z0-9đĐ]*(?:,\\s*\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)*(?:\\s+và\\s+\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*)?', text, re.IGNORECASE)\n",
    "\n",
    "    # Process joined \"Điều\" lists\n",
    "    for item in joined_dieu_list:\n",
    "        # Find all \"Điều\" references in the joined list\n",
    "        joined_dieu_refs = re.findall(r'\\d+[a-zA-Z0-9đĐ]*(?:\\.\\d+[a-zA-Z0-9đĐ]*)*', item)\n",
    "        # Add only the unique \"Điều\" references to the main list\n",
    "        for ref in joined_dieu_refs:\n",
    "            if ref not in dieu_list:\n",
    "                dieu_list.append(ref)\n",
    "    return dieu_list\n",
    "\n",
    "# Hàm trích xuất khoản\n",
    "def extract_khoan(text):\n",
    "    pattern = r'khoản\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s*| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_khoan_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if '-' in part:\n",
    "                start, end = part.split('-')\n",
    "                for idx in range(int(start), int(end) + 1):\n",
    "                    final_khoan_list.append(str(idx))\n",
    "            elif len(part) == 1 and part.isalpha() or part[0].isdigit():\n",
    "                final_khoan_list.append(part)\n",
    "    return final_khoan_list\n",
    "\n",
    "# Hàm trích xuất điểm\n",
    "def extract_diem(text):\n",
    "    pattern = r'điểm\\s+((?:[a-zA-Z0-9đĐ]+(?:\\.\\d+)?(?:,\\s+| và |, và |))*[a-zA-Z0-9đĐ]+(?:\\.\\d+)?)(?=\\s|,|\\.|;|$|\\s+và\\s+[a-zA-ZđĐ0-9])'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    final_diem_list = []\n",
    "    for match in matches:\n",
    "        parts = re.split(r',\\s*|\\s+và\\s+', match)\n",
    "        for part in parts:\n",
    "            part = part.strip().strip('.').lower()\n",
    "            if re.match(r'^[a-zA-ZđĐ]\\d*(\\.\\d+)*$', part) or re.match(r'^\\d+(\\.\\d+)*$', part):\n",
    "                final_diem_list.append(part)\n",
    "    return final_diem_list\n",
    "\n",
    "# hàm xử lý đầu ra ccpl\n",
    "def processing_output_ccpl(input_data):\n",
    "    output_array = []\n",
    "    for dieu in input_data:\n",
    "        dieu_value = \", \".join(dieu[\"Dieu\"])\n",
    "        if dieu[\"Khoan\"]:\n",
    "            for khoan in dieu[\"Khoan\"]:\n",
    "                for khoan_item in khoan[\"Khoan\"]:\n",
    "                    if khoan[\"Diem\"]:\n",
    "                        for diem in khoan[\"Diem\"]:\n",
    "                            for diem_item in diem:\n",
    "                                output_array.extend([\n",
    "                                    dieu_value,\n",
    "                                    khoan_item,\n",
    "                                    diem_item\n",
    "                                ])\n",
    "                    else:\n",
    "                        output_array.extend([\n",
    "                            dieu_value,\n",
    "                            khoan_item,\n",
    "                            \"0\"\n",
    "                        ])\n",
    "        else:\n",
    "            output_array.extend([\n",
    "                dieu_value,\n",
    "                \"0\",\n",
    "                \"0\"\n",
    "            ])\n",
    "    if output_array:\n",
    "        return output_array\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to remove duplicate ccpl entries\n",
    "def remove_duplicates_ccpls(data):\n",
    "    for item in data:\n",
    "        seen = set()\n",
    "        unique_ccpls = []\n",
    "        for ccpl in item['ccpls']:\n",
    "            if ccpl['Dieu']:  # Kiểm tra nếu 'Dieu' không phải là rỗng\n",
    "                ccpl_tuple = (ccpl['LawID'], ccpl['LawTitle'], ccpl['Dieu'], ccpl['Khoan'], ccpl['Diem'])\n",
    "                if ccpl_tuple not in seen:\n",
    "                    seen.add(ccpl_tuple)\n",
    "                    unique_ccpls.append(ccpl)\n",
    "        item['ccpls'] = unique_ccpls\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def post_request(url, headers):\n",
    "    response = requests.post(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def post_request_key(url, headers, data):\n",
    "    try:\n",
    "        data = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        data.raise_for_status()  # Raise an error for bad status codes\n",
    "        return data.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng hàm để thực hiện yêu cầu POST\n",
    "def get_api():\n",
    "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "    headers1 = {\n",
    "        'Cookie': 'Culture=vi; Culture=vi'\n",
    "    }\n",
    "\n",
    "    key = post_request(url1, headers1)\n",
    "\n",
    "    url = 'https://apids.thuvienphapluat.vn/data/get-danluat'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "        'Content-Type': 'application/json',\n",
    "        'Cookie': 'Culture=vi'\n",
    "    }\n",
    "    return url, headers\n",
    "\n",
    "def send_data_to_api(data):\n",
    "    url, headers = get_api()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lọc các thẻ câu hỏi, thẻ đầu tiên là thẻ nào sẽ lấy dạng của thẻ đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the order of tags\n",
    "def get_tag_order(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    for temp in soup.find_all('em'):\n",
    "        temp.decompose()\n",
    "    for temp in soup.find_all('p', dir=\"ltr\", style=\"line-height: 1.8; margin-top: 10pt; margin-bottom: 10pt; text-align: center;\"):\n",
    "        temp.decompose()\n",
    "    for temp in soup.find_all('p', style=\"font-family: Arial, sans-serif; background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\"):\n",
    "        temp.decompose()\n",
    "    # xóa các mục tải về\n",
    "    for download_tag in soup.find_all('p'):\n",
    "        for download_tag1 in download_tag.find_all('a'):\n",
    "            download_tag.decompose()\n",
    "    for element in soup.descendants:\n",
    "        if isinstance(element, Tag):\n",
    "            if element.name == 'h2':\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'h1':\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color:#ff0000;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color:#006400;\":\n",
    "                return element.name, element.get('style')\n",
    "            # elif element.name == 'span' and element.get('style') == \"color:#800000;\":\n",
    "            #     print(element.name, element.get('style'))\n",
    "            #     return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color:#000000;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color: rgb(56, 118, 29); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color: rgb(56, 118, 29); background-color: transparent; font-weight: 700; font-style: italic; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color: rgb(0, 0, 0); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; white-space-collapse: preserve;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"font-family: Arial, sans-serif; background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"background-color: transparent; color: rgb(56, 118, 29); font-weight: 700; white-space: pre-wrap; font-family: arial, helvetica, sans-serif; font-size: 14px; text-align: justify;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"background-color: transparent; color: rgb(0, 0, 0); font-weight: 700; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style')  \n",
    "            elif element.name == 'span' and element.get('style') == 'background-color: transparent; color: rgb(56, 118, 29); font-family: Arial, sans-serif; font-weight: 700; white-space: pre-wrap;':\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"font-size: 14pt; font-family: Arial, sans-serif; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style') \n",
    "            elif element.name == 'span' and element.get('style') == \"font-size: 11pt; color: rgb(0, 0, 0); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-variant-position: normal; vertical-align: baseline; white-space-collapse: preserve;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"background-color: transparent; color: rgb(0, 0, 0); white-space-collapse: preserve; font-family: arial, helvetica, sans-serif; font-size: 14px;\":\n",
    "                return element.name, element.get('style') \n",
    "            elif element.name == 'span' and element.get('style') == \"color: rgb(56, 118, 29); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"background-color: transparent; color: rgb(0, 0, 0); font-weight: 700; white-space: pre-wrap; font-family: arial, helvetica, sans-serif; font-size: 14px; text-align: justify;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color: rgb(33, 37, 41); font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color: rgb(0, 0, 0); background-color: transparent; font-weight: 700; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                return element.name, element.get('style')\n",
    "            \n",
    "            elif element.name == 'p' and element.get('style') == \"text-align: justify;\":\n",
    "                if (element.find('strong') or element.find('b')) and not element.find('em'):\n",
    "                    return element.name, element.get('style')\n",
    "            elif element.name == 'div' and element.get('style') == \"text-align: justify;\":\n",
    "                if (element.find('strong') or element.find('b')) and not element.find('em'):\n",
    "                    return element.name, element.get('style')\n",
    "            elif element.name == 'p' and element.get('style') == \"line-height: 1.27636; margin-top: 0pt; margin-bottom: 0pt; padding: 0pt 0pt 6pt; text-align: justify;\":     \n",
    "                if element.get('dir') == 'ltr' and (element.find('strong') or element.find('b')) and not element.find('em'):\n",
    "                    return element.name, element.get('style')\n",
    "            elif element.name == 'p' and element.get('style') == None:\n",
    "                if (element.find('strong') or element.find('b')) and not element.find('em'):\n",
    "                    return element.name, element.get('style')\n",
    "            elif element.name == 'span' and element.get('style') == \"color: rgb(0, 0, 0); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;\":\n",
    "                if (element.find('strong') or element.find('b')) and not element.find('em'):\n",
    "                    return element.name, element.get('style') \n",
    "            \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each input and determine the order of tags\n",
    "def processing_question(data_input):\n",
    "    questions = []\n",
    "    tag_name, style_name = get_tag_order(data_input['content'])\n",
    "    soup = BeautifulSoup(data_input['content'], 'html.parser')\n",
    "    # print(tag_name)\n",
    "    for temp in soup.find_all('em'):\n",
    "        temp.decompose()\n",
    "    for temp in soup.find_all('p', dir=\"ltr\", style=\"line-height: 1.8; margin-top: 10pt; margin-bottom: 10pt; text-align: center;\"):\n",
    "        temp.decompose()\n",
    "    # xóa các mục tải về\n",
    "    for download_tag in soup.find_all('p'):\n",
    "        for download_tag1 in download_tag.find_all('a'):\n",
    "            download_tag.decompose()\n",
    "    if tag_name =='h2' or tag_name =='h1':\n",
    "        for question_tag in soup.find_all(tag_name):\n",
    "            for temp1 in question_tag:\n",
    "                questions.append(temp1.text)\n",
    "    elif (tag_name == 'p' and style_name == 'text-align: justify;') or (tag_name == 'div' and style_name == 'text-align: justify;') or (tag_name == 'p' and style_name == None) or (tag_name == 'span' and style_name == 'color: rgb(0, 0, 0); background-color: transparent; font-variant-numeric: normal; font-variant-east-asian: normal; vertical-align: baseline; white-space: pre-wrap;'):\n",
    "        for span in soup.find_all('span', id= True):\n",
    "            children = [child for child in span.find_all(recursive=False) if child.name is not None]\n",
    "            if len(children) == 1:\n",
    "                for question_tag in span.find_all(tag_name, style = style_name):\n",
    "                    if (question_tag.find('strong') or question_tag.find('b')) and not question_tag.find('em'):\n",
    "                        for temp1 in question_tag:\n",
    "                            is_within_table = False\n",
    "                            parent = temp1.parent\n",
    "                            while parent:\n",
    "                                if parent.name == 'table':\n",
    "                                    is_within_table = True\n",
    "                                    break\n",
    "                                parent = parent.parent\n",
    "                            if not is_within_table:\n",
    "                                questions.append(temp1.text)\n",
    "    # (tag_name == 'p' and style_name == 'line-height: 1.27636; margin-top: 0pt; margin-bottom: 0pt; padding: 0pt 0pt 6pt; text-align: justify;')\n",
    "    elif tag_name:\n",
    "        for span in soup.find_all('span', id= True):\n",
    "            children = [child for child in span.find_all(recursive=False) if child.name is not None]\n",
    "            if len(children) == 1:\n",
    "                for question_tag in span.find_all(tag_name, style = style_name):\n",
    "                    for temp1 in question_tag:\n",
    "                        is_within_table = False\n",
    "                        parent = temp1.parent\n",
    "                        while parent:\n",
    "                            if parent.name == 'table':\n",
    "                                is_within_table = True\n",
    "                                break\n",
    "                            parent = parent.parent\n",
    "                        if not is_within_table:\n",
    "                            questions.append(temp1.text)\n",
    "                # print(question_tag)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_answer(data_input, questions):\n",
    "    processed_questions = set()  # Set để lưu các câu hỏi đã được xử lý\n",
    "    questions_and_answers1 = []\n",
    "    soup = BeautifulSoup(data_input['content'], 'html.parser')\n",
    "    \n",
    "    # xóa các mục tải về\n",
    "    for download_tag in soup.find_all('p'):\n",
    "        for download_tag1 in download_tag.find_all('a'):\n",
    "            if download_tag1 is not None and 'href' in download_tag1:\n",
    "                href = download_tag1.get('href')\n",
    "                if href and href.endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')):\n",
    "                    download_tag.decompose()\n",
    "    data_answer = soup.find_all()\n",
    "    for i in range(len(questions)):\n",
    "        red_flag = True\n",
    "        temp_results = []\n",
    "        question_current = questions[i]\n",
    "        if i + 1 < len(questions):\n",
    "            next_question = questions[i+1]\n",
    "        else:\n",
    "            next_question = 'Xuan Vinh Gay'\n",
    "        patterns = r\"^\\d+\\.\\s*|^\\(\\d+\\)\\s*|^\\d/\\s*\"\n",
    "        # Tìm và thay thế phần (1) và phần 1. bằng chuỗi rỗng\n",
    "        final_question = re.sub(patterns, \"\", question_current).strip()\n",
    "        for temp_question in data_answer:\n",
    "            normalized_text = unicodedata.normalize(\"NFC\", temp_question.text)\n",
    "            if i < len(questions) and normalized_text == questions[i] and normalized_text not in processed_questions:\n",
    "                processed_questions.add(normalized_text)  # Thêm câu hỏi vào set\n",
    "                answer_text = \"\"\n",
    "                parent_p = temp_question.find_parent('p')\n",
    "                # print(parent_p)\n",
    "                if parent_p:\n",
    "                    next_tag1 = parent_p.find_next_sibling()\n",
    "                    while next_tag1 and red_flag:\n",
    "                        if next_tag1.text.strip() == unicodedata.normalize(\"NFC\", next_question).strip():\n",
    "                            red_flag = False\n",
    "                        elif red_flag:\n",
    "                            answer = unicodedata.normalize(\"NFC\", next_tag1.text).strip()\n",
    "                            # print(answer)\n",
    "                            # print(answer.encode('utf-8'))\n",
    "                            answer_text += answer + \"\\n\"\n",
    "                            # next_tag1 = next_tag1.find_next_sibling()\n",
    "                            if next_tag1.name and red_flag:\n",
    "                                #  == 'p'\n",
    "                                p_tag = next_tag1.text\n",
    "                                if next_tag1.find('a') and not next_tag1.text.startswith('>>'):\n",
    "                                    for a_tag in next_tag1.find_all('a'):\n",
    "                                        href = a_tag.get('href')\n",
    "                                        if href and '.aspx' in href:\n",
    "                                            final = processing_ccpl_full(a_tag.text)\n",
    "                                            positions = split_by_dieu(final)\n",
    "                                            # test = p_tag.split(positions)\n",
    "                                            # print(positions)\n",
    "                                            if positions:\n",
    "                                                lawtitle = positions[-1]\n",
    "                                            else:\n",
    "                                                lawtitle = \"default_value\"  # Hoặc một giá trị mặc định nếu positions rỗng\n",
    "\n",
    "                                            # Kiểm tra nếu lawtitle không rỗng trước khi gọi split()\n",
    "                                            if lawtitle:\n",
    "                                                test = p_tag.split(lawtitle)\n",
    "                                            else:\n",
    "                                                # Xử lý khi lawtitle rỗng (nếu cần thiết)\n",
    "                                                test = []\n",
    "                                            # print(lawtitle)\n",
    "                                            anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                            anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                            if test and \"điều\" in test[0].lower():\n",
    "                                                p_tag = p_tag.replace(test[0], '', 1)\n",
    "                                                final = processing_ccpl_full(test[0])\n",
    "                                                positions = split_by_dieu(final)\n",
    "                                                data_ccpl = []\n",
    "                                                for dieu in positions:\n",
    "                                                        if \"điều\" in dieu.lower():\n",
    "                                                            dieu_dict = {\"Dieu\": extract_dieu(dieu), \"Khoan\": []}\n",
    "                                                            # Tìm anchor trong href\n",
    "                                                            khoan_list = split_by_khoan(dieu)\n",
    "                                                            for khoan in khoan_list:\n",
    "                                                                if \"khoản\" in khoan.lower():\n",
    "                                                                    khoan_dict = {\"Khoan\": extract_khoan(khoan), \"Diem\": []}\n",
    "                                                                    if \"điểm\" in khoan.lower():\n",
    "                                                                        diem_dict = extract_diem(khoan)\n",
    "                                                                        khoan_dict[\"Diem\"].append(diem_dict)\n",
    "                                                                    dieu_dict[\"Khoan\"].append(khoan_dict)\n",
    "                                                            data_ccpl.append(dieu_dict)\n",
    "                                                            # print(data_ccpl)\n",
    "                                                            # Tạo một từ điển mới và gán giá trị từ mảng\n",
    "                                                            ccpl_temp = processing_output_ccpl(data_ccpl)\n",
    "                                                            if len(ccpl_temp) > 3:\n",
    "                                                                for i in range(0, len(ccpl_temp), 3):\n",
    "                                                                    # Lấy 3 phần tử từ chỉ mục i đến i+3 (không bị lỗi nếu kích thước không đủ)\n",
    "                                                                    chunk = ccpl_temp[i:i+3]\n",
    "                                                                    anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                                    law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                                    anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                                    law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                                    # print(ccpl_temp)\n",
    "                                                                    if len(chunk) > 0:\n",
    "                                                                        dictionary = {\n",
    "                                                                            \"url\": href,\n",
    "                                                                            \"anchor\": anchor,\n",
    "                                                                            \"LawID\": law_id,\n",
    "                                                                            \"LawTitle\": lawtitle,\n",
    "                                                                            \"Dieu\": chunk[0],\n",
    "                                                                            \"Khoan\": chunk[1],\n",
    "                                                                            \"Diem\": chunk[2]\n",
    "                                                                        }\n",
    "                                                                        temp_results.append(dictionary)\n",
    "                                                            else:\n",
    "                                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                                if len(ccpl_temp) > 0:\n",
    "                                                                    dictionary = {\n",
    "                                                                        \"url\": href,\n",
    "                                                                        \"anchor\": anchor,\n",
    "                                                                        \"LawID\": law_id,\n",
    "                                                                        \"LawTitle\": lawtitle,\n",
    "                                                                        \"Dieu\": ccpl_temp[0] or \"0\",\n",
    "                                                                        \"Khoan\": ccpl_temp[1],\n",
    "                                                                        \"Diem\": ccpl_temp[2]\n",
    "                                                                    }\n",
    "                                                                    temp_results.append(dictionary)\n",
    "\n",
    "                                            else:\n",
    "                                                temp_else = {}\n",
    "                                                anchor_match = re.search(r'anchor=(\\w+)', href)\n",
    "                                                law_id_match = re.search(r'-(\\d+)\\.aspx', href)\n",
    "                                                anchor = anchor_match.group(1) if anchor_match else \"0\"\n",
    "                                                law_id = law_id_match.group(1) if law_id_match else \"0\"\n",
    "                                                # law_title = a_tag.get_text()\n",
    "                                                temp_else[\"url\"] = href\n",
    "                                                temp_else[\"anchor\"] = anchor\n",
    "                                                temp_else[\"LawID\"] = law_id\n",
    "                                                temp_else[\"LawTitle\"] = lawtitle\n",
    "                                                temp_else[\"Dieu\"] = \"0\"\n",
    "                                                temp_else[\"Khoan\"] = \"0\"\n",
    "                                                temp_else[\"Diem\"] = \"0\"\n",
    "                                                temp_results.append(temp_else)\n",
    "                            next_tag1 = next_tag1.find_next_sibling()\n",
    "                ccpls = []\n",
    "                # ccpls.extend(temp_results)\n",
    "                ccpls.extend(temp_results)\n",
    "                questions_and_answers1.append({\n",
    "                    \"question\": unicodedata.normalize(\"NFC\", final_question),\n",
    "                    \"answer\": re.sub(r'\\n{2,}', '\\n', answer_text.strip()),\n",
    "                    \"links\": [],\n",
    "                    \"ccpls\": ccpls\n",
    "                    \n",
    "                })\n",
    "    patterns = [\n",
    "        re.compile(r\"\\s*\\n{2}\\s*Về\\s*vấn\\s*đề\\s*này,\"),  # Khớp với ít nhất 2 ký tự xuống dòng trước và giữa các từ\n",
    "        re.compile(r\"Về vấn đề này,\\s*THƯ VIỆN PHÁP LUẬT\\s*giải đáp như sau:\"),\n",
    "        re.compile(r\"\\*\\*\\*\\*\", re.MULTILINE),\n",
    "        re.compile(r\"THƯ VIỆN PHÁP LUẬT\"),\n",
    "        re.compile(r\"^Xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"^>> Xem thêm.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Xem chi tiết tại.*\", re.MULTILINE),\n",
    "        re.compile(r\"^>> Tham khảo.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Tham khảo.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Chọn lĩnh vực để xem văn bản liên quan.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Trân trọng.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Bài được viết theo.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Xem toàn bộ.*\", re.MULTILINE),\n",
    "        re.compile(r\"^Xem bài viết.*\", re.MULTILINE)\n",
    "    ]\n",
    "\n",
    "    # Vòng lặp qua từng cặp câu hỏi và câu trả lời\n",
    "    for qa_pair in questions_and_answers1:\n",
    "        # Lấy câu trả lời từ cặp câu hỏi và câu trả lời\n",
    "        answer = qa_pair[\"answer\"]\n",
    "        # Thay thế từ \"tại đây\" thành rỗng trong câu trả lời\n",
    "        cleaned_answer = re.sub(r'.*>\\s*tại đây\\s*<.*', '', answer, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        # Cập nhật câu trả lời đã được thay thế trong cặp câu hỏi và câu trả lời\n",
    "        qa_pair[\"answer\"] = cleaned_answer\n",
    "\n",
    "    for item in questions_and_answers1:\n",
    "        answer_text = item[\"answer\"]\n",
    "        # Khởi tạo start_index với giá trị -1 (không tìm thấy)\n",
    "        start_index = -1\n",
    "        # Kiểm tra từng mẫu regex và cập nhật start_index nếu tìm thấy\n",
    "        for pattern in patterns:\n",
    "            match = pattern.search(answer_text)\n",
    "            if match:\n",
    "                index = match.start()\n",
    "                # Nếu start_index chưa được cập nhật hoặc tìm thấy index nhỏ hơn (gần đầu chuỗi hơn)\n",
    "                if start_index == -1 or index < start_index:\n",
    "                    start_index = index\n",
    "\n",
    "        # Nếu tìm thấy bất kỳ mẫu nào, xóa từ đoạn đó đến hết câu trả lời\n",
    "        if start_index != -1:\n",
    "            cleaned_answer = answer_text[:start_index].strip()\n",
    "            item[\"answer\"] = cleaned_answer\n",
    "    # In ra danh sách câu hỏi và câu trả lời đã được làm sạch\n",
    "    return json.dumps(questions_and_answers1, ensure_ascii=False, indent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_link(data_input, questions):\n",
    "    processed_questions = set()  # Set để lưu các câu hỏi đã được xử lý\n",
    "    link = []\n",
    "    soup = BeautifulSoup(data_input['content'], 'html.parser')\n",
    "    data_answer = soup.find_all()\n",
    "    for i in range(len(questions)):\n",
    "        red_flag = True\n",
    "        if i + 1 < len(questions):\n",
    "            next_question = questions[i+1]\n",
    "        else:\n",
    "            next_question = 'Xuan Vinh Gay'\n",
    "        for temp_question in data_answer:\n",
    "            normalized_text = unicodedata.normalize(\"NFC\", temp_question.text)\n",
    "            if normalized_text == questions[i] and normalized_text not in processed_questions:\n",
    "                # print('ok')\n",
    "                processed_questions.add(normalized_text)  # Thêm câu hỏi vào set\n",
    "                # Tạo danh sách temp_results mới cho mỗi thẻ <h2>\n",
    "                temp_results = []\n",
    "                parent_p = temp_question.find_parent()\n",
    "                # print(parent_p)\n",
    "                if parent_p:\n",
    "                    next_tag1 = parent_p.find_next_sibling()\n",
    "                    while next_tag1 and red_flag:\n",
    "                        if next_tag1.text.strip() == unicodedata.normalize(\"NFC\", next_question).strip():\n",
    "                            red_flag = False\n",
    "                        if red_flag and next_tag1:\n",
    "                            link_tag = next_tag1.find_all('a')\n",
    "                            if link_tag:\n",
    "                                for a_tag in link_tag:\n",
    "                                    if a_tag is not None and 'href' in a_tag:\n",
    "                                        print(a_tag.get('href'))\n",
    "                                        href = a_tag.get('href')\n",
    "                                        # print(href)\n",
    "                                        if href:\n",
    "                                            links = {}\n",
    "                                            href = href.strip()\n",
    "                                            if not any(link['url'] == href for link in temp_results) and href.endswith(('.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.pdf', '.png')):\n",
    "                                                for temp in next_tag1.find_all('a'):\n",
    "                                                    temp.decompose()\n",
    "                                                links['title'] = next_tag1.text.strip() #a_tag\n",
    "                                                links['url'] = href\n",
    "                                                # print(links['title'])\n",
    "                                                # print(links['url'])\n",
    "                                                temp_results.append(links)\n",
    "                            next_tag1 = next_tag1.find_next_sibling()\n",
    "                link.append({\n",
    "                    # \"question\": h2_tag.text.strip(),\n",
    "                    \"links\": temp_results\n",
    "                })\n",
    "    return json.dumps(link, ensure_ascii=False, indent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_input, questions):\n",
    "    links_data = json.loads(processing_link(data_input, questions))\n",
    "    file1_data = json.loads(processing_answer(data_input, questions))\n",
    "\n",
    "    # ghép link\n",
    "    for index, item in enumerate(file1_data):\n",
    "        item['links'] = links_data[index]['links']\n",
    "    # Chỉ giữ lại các mục có câu trả lời\n",
    "    filtered_questions_and_answers = [qa for qa in file1_data if 'answer' in qa and qa['answer']]\n",
    "    filtered_questions_and_answers1 = [qa for qa in filtered_questions_and_answers if 'question' in qa and qa['question']]\n",
    "    questions_and_answers_final = []\n",
    "    for qa in filtered_questions_and_answers1:\n",
    "        question_text = qa[\"question\"]\n",
    "        # Kiểm tra xem câu hỏi có chứa các từ \"dự thảo\", \"đề xuất\", \"sắp tới\" hoặc \"dự kiến\" không\n",
    "        if not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', question_text.lower(), flags=re.IGNORECASE) and not question_text.lower().startswith(\"đã có\"):\n",
    "            questions_and_answers_final.append(qa)\n",
    "\n",
    "    # Remove duplicates\n",
    "    questions_and_answers_final = remove_duplicates_ccpls(questions_and_answers_final)\n",
    "    return questions_and_answers_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153964\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n",
      "{\"messages\":\"success\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 37\u001b[0m\n\u001b[0;32m     30\u001b[0m headers_post \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m key[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccessToken\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCookie\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCulture=vi\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m }\n\u001b[0;32m     36\u001b[0m url, headers \u001b[38;5;241m=\u001b[39m get_api()\n\u001b[1;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpost_request_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# print(response.get('data'))\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m, in \u001b[0;36mpost_request_key\u001b[1;34m(url, headers, data)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_request_key\u001b[39m(url, headers, data):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         data\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an error for bad status codes\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\My_Pc\\anaconda3\\envs\\Minh-AI\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_api = {\n",
    "    \"page\": 1, #14039\n",
    "    \"num\": 1\n",
    "}\n",
    "url, headers = get_api()\n",
    "response = post_request_key(url, headers, data_api)\n",
    "file_note = []\n",
    "data = []\n",
    "if response:\n",
    "    specific_page = response.get('total')\n",
    "    print(specific_page)\n",
    "# # Số trang cần lấy\n",
    "# specific_page =  298847 #46146 pldn ko có tbvbm\n",
    "# print(now)\n",
    "num = 100\n",
    "total_page = specific_page // num + ( 1 if specific_page % num > 0 else 0)\n",
    "for num_page in range(total_page,1, -1):\n",
    "    data_api = {\n",
    "        \"page\": num_page, #14039\n",
    "        \"num\": num\n",
    "    }\n",
    "    url1 = 'https://apids.thuvienphapluat.vn/auth/get-token?key=pvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQS563xADXH'\n",
    "    headers1 = {\n",
    "        'Cookie': 'Culture=vi; Culture=vi'\n",
    "    }\n",
    "\n",
    "    key = post_request(url1, headers1)\n",
    "    url_post = \"https://apids.thuvienphapluat.vn/crud/log-extracted-news-data\"\n",
    "\n",
    "    headers_post = {\n",
    "    'Authorization': 'Bearer ' + key['Data']['AccessToken'],\n",
    "    'Content-Type': 'application/json',\n",
    "    'Cookie': 'Culture=vi'\n",
    "    }\n",
    "\n",
    "    url, headers = get_api()\n",
    "    response = post_request_key(url, headers, data_api)\n",
    "    # print(response.get('data'))\n",
    "    if response:\n",
    "        data_input = response.get('data')\n",
    "        for temp in data_input:\n",
    "            # print(temp[\"content\"])\n",
    "            output = extract_data(temp, processing_question(temp))\n",
    "            # print(extract_data(temp[\"content\"]))\n",
    "            if output != [] and not re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "                dictionary = {\n",
    "                    \"objid\": int(temp['obj_id']),\n",
    "                    \"source\": str(temp['obj_code']),\n",
    "                    \"data\": json.dumps(output, ensure_ascii=False),\n",
    "                    \"type\": 1\n",
    "                }\n",
    "                dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "                data.append(dictionary)\n",
    "                response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "                print(response.text)\n",
    "                # time.sleep(1)\n",
    "            elif output != [] and re.search(r'\\b(?:dự thảo|đề xuất|sắp tới|dự kiến|đáp án|tra cứu điểm thi)\\b', temp[\"title\"].lower(), flags=re.IGNORECASE):\n",
    "                dictionary = {\n",
    "                    \"objid\": int(temp['obj_id']),\n",
    "                    \"source\": str(temp['obj_code']),\n",
    "                    \"data\": json.dumps(output, ensure_ascii=False),\n",
    "                    \"type\": 3\n",
    "                }\n",
    "                dictionary = json.dumps(dictionary, ensure_ascii=False)\n",
    "                data.append(dictionary)\n",
    "                response = requests.request(\"POST\", url_post, headers=headers_post, data=dictionary)\n",
    "                print(response.text)\n",
    "                # time.sleep(1)\n",
    "            else:\n",
    "                file_note.append(temp['obj_id'])\n",
    "            \n",
    "now = specific_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_miss.txt', 'w') as file:\n",
    "#     for item in data_miss:\n",
    "#         file.write(f\"{item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Minh-AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
